==================================================
FINETUNING PARAMETERS:
base model: projecte-aina/aguila-7b
--------------------------------------------------
train_split: [:20000]
dataset_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/en-es_europarl-unpc/europarl-unpc_en-es_bidir.jsonl
validation_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/flores_dev_eng-spa.jsonl
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/flores_dev_spa-eng.jsonl
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/unpc_dev_en-es_unidir.jsonl
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/unpc_dev_es-en_unidir.jsonl
--------------------------------------------------
output_dir: /fs/surtr0/jprats/models/checkpoints/tr4_aguila_qlora_en-es10k_ebs256-4x1x64_linear_lr2e-4_20231209-10.45.40
--------------------------------------------------
learning_rate: 0.0001
lr_scheduler_type: linear
effective batch size: 64
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 64
  CUDA Devices: 2,3,4,5
num_train_epochs: 3
warmup_ratio: 0.03
group_by_length: False
evaluation_strategy: steps
eval_steps: 0.11111
--------------------------------------------------
lora_r: 16
lora_alpha: 16
--------------------------------------------------
bf16: True
--------------------------------------------------
use_4bit: True
bnb_4bit_quant_type: nf4
bnb_4bit_compute_dtype: float16
==================================================
==================================================
FINETUNING PARAMETERS:==================================================

base model:FINETUNING PARAMETERS:
 base model:projecte-aina/aguila-7b
 --------------------------------------------------projecte-aina/aguila-7b

train_split:-------------------------------------------------- 
[:20000]train_split:
 dataset_files:[:20000]

	/fs/surtr0/jprats/data/processed/04-finetuning/en-es_europarl-unpc/europarl-unpc_en-es_bidir.jsonldataset_files:

validation_files:	/fs/surtr0/jprats/data/processed/04-finetuning/en-es_europarl-unpc/europarl-unpc_en-es_bidir.jsonl

	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/flores_dev_eng-spa.jsonlvalidation_files:

	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/flores_dev_spa-eng.jsonl	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/flores_dev_eng-spa.jsonl

	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/unpc_dev_en-es_unidir.jsonl	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/flores_dev_spa-eng.jsonl

	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/unpc_dev_es-en_unidir.jsonl	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/unpc_dev_en-es_unidir.jsonl

--------------------------------------------------	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/unpc_dev_es-en_unidir.jsonl

output_dir:-------------------------------------------------- 
/fs/surtr0/jprats/models/checkpoints/tr4_aguila_qlora_en-es10k_ebs256-4x1x64_linear_lr2e-4_20231209-10.45.40output_dir:
 --------------------------------------------------/fs/surtr0/jprats/models/checkpoints/tr4_aguila_qlora_en-es10k_ebs256-4x1x64_linear_lr2e-4_20231209-10.45.40

learning_rate:-------------------------------------------------- 
learning_rate: 0.0001
0.0001lr_scheduler_type:
 lr_scheduler_type:linear 
lineareffective batch size:
 effective batch size:64 
64  per_device_train_batch_size:
   per_device_train_batch_size:1 
1  gradient_accumulation_steps:
   gradient_accumulation_steps:64 
64
  CUDA Devices:   CUDA Devices:2,3,4,5 
2,3,4,5num_train_epochs:
 num_train_epochs:3 
3warmup_ratio:
 warmup_ratio:0.03 
0.03group_by_length:
 group_by_length:False 
Falseevaluation_strategy:
 evaluation_strategy:steps 
stepseval_steps:
 eval_steps:0.11111 
0.11111--------------------------------------------------

--------------------------------------------------lora_r:
 lora_r:16 
16lora_alpha:
 lora_alpha:16 
16--------------------------------------------------

--------------------------------------------------bf16:
 bf16:True 
True--------------------------------------------------

--------------------------------------------------use_4bit:
 use_4bit:True 
Truebnb_4bit_quant_type:
 bnb_4bit_quant_type:nf4 
nf4bnb_4bit_compute_dtype:
 bnb_4bit_compute_dtype:float16 
float16==================================================

==================================================
==================================================
FINETUNING PARAMETERS:
base model: projecte-aina/aguila-7b
--------------------------------------------------
train_split: [:20000]
dataset_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/en-es_europarl-unpc/europarl-unpc_en-es_bidir.jsonl
validation_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/flores_dev_eng-spa.jsonl
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/flores_dev_spa-eng.jsonl
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/unpc_dev_en-es_unidir.jsonl
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/unpc_dev_es-en_unidir.jsonl
--------------------------------------------------
output_dir: /fs/surtr0/jprats/models/checkpoints/tr4_aguila_qlora_en-es10k_ebs256-4x1x64_linear_lr2e-4_20231209-10.45.40
--------------------------------------------------
learning_rate: 0.0001
lr_scheduler_type: linear
effective batch size: 64
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 64
  CUDA Devices: 2,3,4,5
num_train_epochs: 3
warmup_ratio: 0.03
group_by_length: False
evaluation_strategy: steps
eval_steps: 0.11111
--------------------------------------------------
lora_r: 16
lora_alpha: 16
--------------------------------------------------
bf16: True
--------------------------------------------------
use_4bit: True
bnb_4bit_quant_type: nf4
bnb_4bit_compute_dtype: float16
==================================================
================================================================================
Your GPU supports bfloat16, you can accelerate training with the argument --bf16
================================================================================
================================================================================
Your GPU supports bfloat16, you can accelerate training with the argument --bf16
================================================================================
================================================================================
Your GPU supports bfloat16, you can accelerate training with the argument --bf16
================================================================================
================================================================================
Your GPU supports bfloat16, you can accelerate training with the argument --bf16
================================================================================
Resulting dataset:
Dataset({
    features: ['text'],
    num_rows: 20000
})
Resulting dataset:
Dataset({
    features: ['text'],
    num_rows: 20000
})
Resulting dataset:
Dataset({
    features: ['text'],
    num_rows: 20000
})
Resulting dataset:
Dataset({
    features: ['text'],
    num_rows: 20000
})
Resulting validation dataset:
Dataset({
    features: ['text'],
    num_rows: 9994
})
Dataset({
    features: ['text'],
    num_rows: 20000
})
False
False
Resulting validation dataset:
Dataset({
    features: ['text'],
    num_rows: 9994
})
Resulting validation dataset:
Dataset({
    features: ['text'],
    num_rows: 9994
})
Dataset({
    features: ['text'],
    num_rows: 20000
})
False
False
Dataset({
    features: ['text'],
    num_rows: 20000
})
False
False
Resulting validation dataset:
Dataset({
    features: ['text'],
    num_rows: 9994
})
Dataset({
    features: ['text'],
    num_rows: 20000
})
False
False
{'loss': 2.0384, 'learning_rate': 1.25e-05, 'epoch': 0.01}
{'loss': 2.0495, 'learning_rate': 2.5e-05, 'epoch': 0.03}
{'loss': 1.9747, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.04}
{'loss': 2.023, 'learning_rate': 5e-05, 'epoch': 0.05}
{'loss': 1.9518, 'learning_rate': 6.25e-05, 'epoch': 0.06}
{'loss': 1.9414, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.08}
{'loss': 2.0244, 'learning_rate': 8.75e-05, 'epoch': 0.09}
{'loss': 1.9696, 'learning_rate': 0.0001, 'epoch': 0.1}
{'loss': 2.0181, 'learning_rate': 9.955752212389381e-05, 'epoch': 0.12}
{'loss': 1.9387, 'learning_rate': 9.911504424778762e-05, 'epoch': 0.13}
{'loss': 1.9164, 'learning_rate': 9.867256637168141e-05, 'epoch': 0.14}
{'loss': 1.885, 'learning_rate': 9.823008849557522e-05, 'epoch': 0.15}
{'loss': 1.811, 'learning_rate': 9.778761061946903e-05, 'epoch': 0.17}
{'loss': 1.8615, 'learning_rate': 9.734513274336283e-05, 'epoch': 0.18}
{'loss': 1.7693, 'learning_rate': 9.690265486725664e-05, 'epoch': 0.19}
{'loss': 1.7545, 'learning_rate': 9.646017699115044e-05, 'epoch': 0.2}
{'loss': 1.7109, 'learning_rate': 9.601769911504426e-05, 'epoch': 0.22}
{'loss': 1.6129, 'learning_rate': 9.557522123893806e-05, 'epoch': 0.23}
{'loss': 1.5692, 'learning_rate': 9.513274336283187e-05, 'epoch': 0.24}
{'loss': 1.4579, 'learning_rate': 9.469026548672566e-05, 'epoch': 0.26}
{'loss': 1.5171, 'learning_rate': 9.424778761061947e-05, 'epoch': 0.27}
{'loss': 1.4035, 'learning_rate': 9.380530973451328e-05, 'epoch': 0.28}
{'loss': 1.3878, 'learning_rate': 9.336283185840709e-05, 'epoch': 0.29}
{'loss': 1.4099, 'learning_rate': 9.29203539823009e-05, 'epoch': 0.31}
{'loss': 1.3058, 'learning_rate': 9.247787610619469e-05, 'epoch': 0.32}
{'loss': 1.3249, 'learning_rate': 9.20353982300885e-05, 'epoch': 0.33}
{'eval_loss': 1.439550757408142, 'eval_runtime': 190.0666, 'eval_samples_per_second': 52.582, 'eval_steps_per_second': 1.647, 'epoch': 0.33}
{'loss': 1.2868, 'learning_rate': 9.15929203539823e-05, 'epoch': 0.35}
{'loss': 1.2723, 'learning_rate': 9.115044247787611e-05, 'epoch': 0.36}
{'loss': 1.2379, 'learning_rate': 9.070796460176992e-05, 'epoch': 0.37}
{'loss': 1.1684, 'learning_rate': 9.026548672566371e-05, 'epoch': 0.38}
{'loss': 1.1419, 'learning_rate': 8.982300884955752e-05, 'epoch': 0.4}
{'loss': 1.0581, 'learning_rate': 8.938053097345133e-05, 'epoch': 0.41}
{'loss': 0.989, 'learning_rate': 8.893805309734515e-05, 'epoch': 0.42}
{'loss': 0.9341, 'learning_rate': 8.849557522123895e-05, 'epoch': 0.44}
{'loss': 0.9028, 'learning_rate': 8.805309734513275e-05, 'epoch': 0.45}
{'loss': 0.8861, 'learning_rate': 8.761061946902655e-05, 'epoch': 0.46}
{'loss': 0.8825, 'learning_rate': 8.716814159292036e-05, 'epoch': 0.47}
{'loss': 0.896, 'learning_rate': 8.672566371681417e-05, 'epoch': 0.49}
{'loss': 0.8951, 'learning_rate': 8.628318584070798e-05, 'epoch': 0.5}
{'loss': 0.9397, 'learning_rate': 8.584070796460177e-05, 'epoch': 0.51}
{'loss': 0.9208, 'learning_rate': 8.539823008849558e-05, 'epoch': 0.52}
{'loss': 0.8559, 'learning_rate': 8.495575221238938e-05, 'epoch': 0.54}
{'loss': 0.9069, 'learning_rate': 8.451327433628319e-05, 'epoch': 0.55}
{'loss': 0.9253, 'learning_rate': 8.4070796460177e-05, 'epoch': 0.56}
{'loss': 0.8989, 'learning_rate': 8.362831858407079e-05, 'epoch': 0.58}
{'loss': 0.9036, 'learning_rate': 8.31858407079646e-05, 'epoch': 0.59}
{'loss': 0.9005, 'learning_rate': 8.274336283185841e-05, 'epoch': 0.6}
{'loss': 0.9176, 'learning_rate': 8.230088495575221e-05, 'epoch': 0.61}
{'loss': 0.8769, 'learning_rate': 8.185840707964602e-05, 'epoch': 0.63}
{'loss': 0.8766, 'learning_rate': 8.141592920353983e-05, 'epoch': 0.64}
{'loss': 0.883, 'learning_rate': 8.097345132743364e-05, 'epoch': 0.65}
{'loss': 0.8738, 'learning_rate': 8.053097345132744e-05, 'epoch': 0.67}
{'eval_loss': 0.9988805055618286, 'eval_runtime': 190.0673, 'eval_samples_per_second': 52.581, 'eval_steps_per_second': 1.647, 'epoch': 0.67}
{'loss': 0.8942, 'learning_rate': 8.008849557522125e-05, 'epoch': 0.68}
{'loss': 0.8629, 'learning_rate': 7.964601769911504e-05, 'epoch': 0.69}
{'loss': 0.936, 'learning_rate': 7.920353982300885e-05, 'epoch': 0.7}
{'loss': 0.8364, 'learning_rate': 7.876106194690266e-05, 'epoch': 0.72}
{'loss': 0.849, 'learning_rate': 7.831858407079647e-05, 'epoch': 0.73}
{'loss': 0.8739, 'learning_rate': 7.787610619469027e-05, 'epoch': 0.74}
{'loss': 0.862, 'learning_rate': 7.743362831858407e-05, 'epoch': 0.76}
{'loss': 0.85, 'learning_rate': 7.699115044247787e-05, 'epoch': 0.77}
{'loss': 0.8751, 'learning_rate': 7.654867256637168e-05, 'epoch': 0.78}
{'loss': 0.9068, 'learning_rate': 7.610619469026549e-05, 'epoch': 0.79}
{'loss': 0.8473, 'learning_rate': 7.56637168141593e-05, 'epoch': 0.81}
{'loss': 0.8596, 'learning_rate': 7.522123893805309e-05, 'epoch': 0.82}
{'loss': 0.8411, 'learning_rate': 7.477876106194691e-05, 'epoch': 0.83}
{'loss': 0.8595, 'learning_rate': 7.433628318584072e-05, 'epoch': 0.84}
{'loss': 0.8543, 'learning_rate': 7.389380530973453e-05, 'epoch': 0.86}
{'loss': 0.8517, 'learning_rate': 7.345132743362832e-05, 'epoch': 0.87}
{'loss': 0.8758, 'learning_rate': 7.300884955752213e-05, 'epoch': 0.88}
{'loss': 0.8752, 'learning_rate': 7.256637168141593e-05, 'epoch': 0.9}
{'loss': 0.8469, 'learning_rate': 7.212389380530974e-05, 'epoch': 0.91}
{'loss': 0.8692, 'learning_rate': 7.168141592920355e-05, 'epoch': 0.92}
{'loss': 0.8632, 'learning_rate': 7.123893805309734e-05, 'epoch': 0.93}
{'loss': 0.8507, 'learning_rate': 7.079646017699115e-05, 'epoch': 0.95}
{'loss': 0.8729, 'learning_rate': 7.035398230088496e-05, 'epoch': 0.96}
{'loss': 0.8512, 'learning_rate': 6.991150442477876e-05, 'epoch': 0.97}
{'loss': 0.8443, 'learning_rate': 6.946902654867257e-05, 'epoch': 0.99}
{'loss': 0.8406, 'learning_rate': 6.902654867256638e-05, 'epoch': 1.0}
{'eval_loss': 0.9439466595649719, 'eval_runtime': 190.094, 'eval_samples_per_second': 52.574, 'eval_steps_per_second': 1.647, 'epoch': 1.0}
{'loss': 0.8529, 'learning_rate': 6.858407079646017e-05, 'epoch': 1.01}
{'loss': 0.8255, 'learning_rate': 6.814159292035398e-05, 'epoch': 1.02}
{'loss': 0.8466, 'learning_rate': 6.76991150442478e-05, 'epoch': 1.04}
{'loss': 0.849, 'learning_rate': 6.725663716814161e-05, 'epoch': 1.05}
{'loss': 0.8544, 'learning_rate': 6.68141592920354e-05, 'epoch': 1.06}
{'loss': 0.8583, 'learning_rate': 6.637168141592921e-05, 'epoch': 1.08}
{'loss': 0.8759, 'learning_rate': 6.592920353982302e-05, 'epoch': 1.09}
{'loss': 0.8586, 'learning_rate': 6.548672566371682e-05, 'epoch': 1.1}
{'loss': 0.836, 'learning_rate': 6.504424778761063e-05, 'epoch': 1.11}
{'loss': 0.8316, 'learning_rate': 6.460176991150442e-05, 'epoch': 1.13}
{'loss': 0.8137, 'learning_rate': 6.415929203539823e-05, 'epoch': 1.14}
{'loss': 0.8629, 'learning_rate': 6.371681415929204e-05, 'epoch': 1.15}
{'loss': 0.8054, 'learning_rate': 6.327433628318585e-05, 'epoch': 1.16}
{'loss': 0.8876, 'learning_rate': 6.283185840707965e-05, 'epoch': 1.18}
{'loss': 0.8634, 'learning_rate': 6.238938053097345e-05, 'epoch': 1.19}
{'loss': 0.8764, 'learning_rate': 6.194690265486725e-05, 'epoch': 1.2}
{'loss': 0.8709, 'learning_rate': 6.150442477876106e-05, 'epoch': 1.22}
{'loss': 0.7954, 'learning_rate': 6.106194690265487e-05, 'epoch': 1.23}
{'loss': 0.8313, 'learning_rate': 6.061946902654868e-05, 'epoch': 1.24}
{'loss': 0.8326, 'learning_rate': 6.017699115044248e-05, 'epoch': 1.25}
{'loss': 0.8351, 'learning_rate': 5.973451327433629e-05, 'epoch': 1.27}
{'loss': 0.832, 'learning_rate': 5.92920353982301e-05, 'epoch': 1.28}
{'loss': 0.8341, 'learning_rate': 5.88495575221239e-05, 'epoch': 1.29}
{'loss': 0.8187, 'learning_rate': 5.8407079646017705e-05, 'epoch': 1.31}
{'loss': 0.8062, 'learning_rate': 5.7964601769911505e-05, 'epoch': 1.32}
{'loss': 0.8201, 'learning_rate': 5.752212389380531e-05, 'epoch': 1.33}
{'eval_loss': 0.9371435642242432, 'eval_runtime': 190.2769, 'eval_samples_per_second': 52.523, 'eval_steps_per_second': 1.645, 'epoch': 1.33}
{'loss': 0.7953, 'learning_rate': 5.707964601769912e-05, 'epoch': 1.34}
{'loss': 0.8296, 'learning_rate': 5.663716814159292e-05, 'epoch': 1.36}
{'loss': 0.8285, 'learning_rate': 5.619469026548673e-05, 'epoch': 1.37}
{'loss': 0.8134, 'learning_rate': 5.575221238938053e-05, 'epoch': 1.38}
{'loss': 0.8042, 'learning_rate': 5.5309734513274336e-05, 'epoch': 1.4}
{'loss': 0.8079, 'learning_rate': 5.486725663716814e-05, 'epoch': 1.41}
{'loss': 0.8055, 'learning_rate': 5.442477876106194e-05, 'epoch': 1.42}
{'loss': 0.8127, 'learning_rate': 5.398230088495575e-05, 'epoch': 1.43}
{'loss': 0.8152, 'learning_rate': 5.3539823008849565e-05, 'epoch': 1.45}
{'loss': 0.7914, 'learning_rate': 5.309734513274337e-05, 'epoch': 1.46}
{'loss': 0.7891, 'learning_rate': 5.265486725663717e-05, 'epoch': 1.47}
{'loss': 0.8064, 'learning_rate': 5.221238938053098e-05, 'epoch': 1.48}
{'loss': 0.8045, 'learning_rate': 5.176991150442479e-05, 'epoch': 1.5}
{'loss': 0.8518, 'learning_rate': 5.132743362831859e-05, 'epoch': 1.51}
{'loss': 0.8492, 'learning_rate': 5.0884955752212395e-05, 'epoch': 1.52}
{'loss': 0.7725, 'learning_rate': 5.0442477876106195e-05, 'epoch': 1.54}
{'loss': 0.8237, 'learning_rate': 5e-05, 'epoch': 1.55}
{'loss': 0.8318, 'learning_rate': 4.955752212389381e-05, 'epoch': 1.56}
{'loss': 0.8378, 'learning_rate': 4.911504424778761e-05, 'epoch': 1.57}
{'loss': 0.82, 'learning_rate': 4.867256637168142e-05, 'epoch': 1.59}
{'loss': 0.8288, 'learning_rate': 4.823008849557522e-05, 'epoch': 1.6}
{'loss': 0.8389, 'learning_rate': 4.778761061946903e-05, 'epoch': 1.61}
{'loss': 0.8034, 'learning_rate': 4.734513274336283e-05, 'epoch': 1.63}
{'loss': 0.8074, 'learning_rate': 4.690265486725664e-05, 'epoch': 1.64}
{'loss': 0.8066, 'learning_rate': 4.646017699115045e-05, 'epoch': 1.65}
{'loss': 0.8099, 'learning_rate': 4.601769911504425e-05, 'epoch': 1.66}
{'eval_loss': 0.9346899390220642, 'eval_runtime': 190.0327, 'eval_samples_per_second': 52.591, 'eval_steps_per_second': 1.647, 'epoch': 1.66}
{'loss': 0.8266, 'learning_rate': 4.5575221238938055e-05, 'epoch': 1.68}
{'loss': 0.8043, 'learning_rate': 4.5132743362831855e-05, 'epoch': 1.69}
{'loss': 0.8583, 'learning_rate': 4.469026548672566e-05, 'epoch': 1.7}
{'loss': 0.7886, 'learning_rate': 4.4247787610619477e-05, 'epoch': 1.72}
{'loss': 0.7928, 'learning_rate': 4.380530973451328e-05, 'epoch': 1.73}
{'loss': 0.7994, 'learning_rate': 4.3362831858407084e-05, 'epoch': 1.74}
{'loss': 0.8047, 'learning_rate': 4.2920353982300885e-05, 'epoch': 1.75}
{'loss': 0.7866, 'learning_rate': 4.247787610619469e-05, 'epoch': 1.77}
{'loss': 0.8124, 'learning_rate': 4.20353982300885e-05, 'epoch': 1.78}
{'loss': 0.8487, 'learning_rate': 4.15929203539823e-05, 'epoch': 1.79}
{'loss': 0.8065, 'learning_rate': 4.115044247787611e-05, 'epoch': 1.8}
{'loss': 0.803, 'learning_rate': 4.0707964601769914e-05, 'epoch': 1.82}
{'loss': 0.7852, 'learning_rate': 4.026548672566372e-05, 'epoch': 1.83}
{'loss': 0.8045, 'learning_rate': 3.982300884955752e-05, 'epoch': 1.84}
{'loss': 0.8198, 'learning_rate': 3.938053097345133e-05, 'epoch': 1.86}
{'loss': 0.8003, 'learning_rate': 3.893805309734514e-05, 'epoch': 1.87}
{'loss': 0.815, 'learning_rate': 3.849557522123894e-05, 'epoch': 1.88}
{'loss': 0.8598, 'learning_rate': 3.8053097345132744e-05, 'epoch': 1.89}
{'loss': 0.7968, 'learning_rate': 3.7610619469026545e-05, 'epoch': 1.91}
{'loss': 0.8002, 'learning_rate': 3.716814159292036e-05, 'epoch': 1.92}
{'loss': 0.8165, 'learning_rate': 3.672566371681416e-05, 'epoch': 1.93}
{'loss': 0.8206, 'learning_rate': 3.628318584070797e-05, 'epoch': 1.95}
{'loss': 0.813, 'learning_rate': 3.5840707964601774e-05, 'epoch': 1.96}
{'loss': 0.8135, 'learning_rate': 3.5398230088495574e-05, 'epoch': 1.97}
{'loss': 0.7969, 'learning_rate': 3.495575221238938e-05, 'epoch': 1.98}
{'loss': 0.7993, 'learning_rate': 3.451327433628319e-05, 'epoch': 2.0}
{'eval_loss': 0.9363567233085632, 'eval_runtime': 190.1252, 'eval_samples_per_second': 52.565, 'eval_steps_per_second': 1.646, 'epoch': 2.0}
{'loss': 0.8222, 'learning_rate': 3.407079646017699e-05, 'epoch': 2.01}
{'loss': 0.7763, 'learning_rate': 3.3628318584070804e-05, 'epoch': 2.02}
{'loss': 0.7995, 'learning_rate': 3.3185840707964604e-05, 'epoch': 2.04}
{'loss': 0.8129, 'learning_rate': 3.274336283185841e-05, 'epoch': 2.05}
{'loss': 0.8055, 'learning_rate': 3.230088495575221e-05, 'epoch': 2.06}
{'loss': 0.8284, 'learning_rate': 3.185840707964602e-05, 'epoch': 2.07}
{'loss': 0.8367, 'learning_rate': 3.1415929203539826e-05, 'epoch': 2.09}
{'loss': 0.8147, 'learning_rate': 3.097345132743363e-05, 'epoch': 2.1}
{'loss': 0.8144, 'learning_rate': 3.0530973451327434e-05, 'epoch': 2.11}
{'loss': 0.8042, 'learning_rate': 3.008849557522124e-05, 'epoch': 2.12}
{'loss': 0.7674, 'learning_rate': 2.964601769911505e-05, 'epoch': 2.14}
{'loss': 0.8211, 'learning_rate': 2.9203539823008852e-05, 'epoch': 2.15}
{'loss': 0.7696, 'learning_rate': 2.8761061946902656e-05, 'epoch': 2.16}
{'loss': 0.8581, 'learning_rate': 2.831858407079646e-05, 'epoch': 2.18}
{'loss': 0.8201, 'learning_rate': 2.7876106194690264e-05, 'epoch': 2.19}
{'loss': 0.8316, 'learning_rate': 2.743362831858407e-05, 'epoch': 2.2}
{'loss': 0.8569, 'learning_rate': 2.6991150442477875e-05, 'epoch': 2.21}
{'loss': 0.7783, 'learning_rate': 2.6548672566371686e-05, 'epoch': 2.23}
{'loss': 0.7966, 'learning_rate': 2.610619469026549e-05, 'epoch': 2.24}
{'loss': 0.7964, 'learning_rate': 2.5663716814159294e-05, 'epoch': 2.25}
{'loss': 0.7958, 'learning_rate': 2.5221238938053098e-05, 'epoch': 2.27}
{'loss': 0.8091, 'learning_rate': 2.4778761061946905e-05, 'epoch': 2.28}
{'loss': 0.796, 'learning_rate': 2.433628318584071e-05, 'epoch': 2.29}
{'loss': 0.7823, 'learning_rate': 2.3893805309734516e-05, 'epoch': 2.3}
{'loss': 0.7918, 'learning_rate': 2.345132743362832e-05, 'epoch': 2.32}
{'loss': 0.7807, 'learning_rate': 2.3008849557522124e-05, 'epoch': 2.33}
{'eval_loss': 0.9359113574028015, 'eval_runtime': 190.1537, 'eval_samples_per_second': 52.557, 'eval_steps_per_second': 1.646, 'epoch': 2.33}
{'loss': 0.77, 'learning_rate': 2.2566371681415928e-05, 'epoch': 2.34}
{'loss': 0.7998, 'learning_rate': 2.2123893805309738e-05, 'epoch': 2.36}
{'loss': 0.801, 'learning_rate': 2.1681415929203542e-05, 'epoch': 2.37}
{'loss': 0.7824, 'learning_rate': 2.1238938053097346e-05, 'epoch': 2.38}
{'loss': 0.7834, 'learning_rate': 2.079646017699115e-05, 'epoch': 2.39}
{'loss': 0.781, 'learning_rate': 2.0353982300884957e-05, 'epoch': 2.41}
{'loss': 0.7825, 'learning_rate': 1.991150442477876e-05, 'epoch': 2.42}
{'loss': 0.7914, 'learning_rate': 1.946902654867257e-05, 'epoch': 2.43}
{'loss': 0.7902, 'learning_rate': 1.9026548672566372e-05, 'epoch': 2.44}
{'loss': 0.7766, 'learning_rate': 1.858407079646018e-05, 'epoch': 2.46}
{'loss': 0.7552, 'learning_rate': 1.8141592920353983e-05, 'epoch': 2.47}
{'loss': 0.7694, 'learning_rate': 1.7699115044247787e-05, 'epoch': 2.48}
{'loss': 0.8007, 'learning_rate': 1.7256637168141594e-05, 'epoch': 2.5}
{'loss': 0.8266, 'learning_rate': 1.6814159292035402e-05, 'epoch': 2.51}
{'loss': 0.8164, 'learning_rate': 1.6371681415929206e-05, 'epoch': 2.52}
{'loss': 0.7641, 'learning_rate': 1.592920353982301e-05, 'epoch': 2.53}
{'loss': 0.8038, 'learning_rate': 1.5486725663716813e-05, 'epoch': 2.55}
{'loss': 0.7928, 'learning_rate': 1.504424778761062e-05, 'epoch': 2.56}
{'loss': 0.8162, 'learning_rate': 1.4601769911504426e-05, 'epoch': 2.57}
{'loss': 0.807, 'learning_rate': 1.415929203539823e-05, 'epoch': 2.59}
{'loss': 0.8005, 'learning_rate': 1.3716814159292036e-05, 'epoch': 2.6}
{'loss': 0.8262, 'learning_rate': 1.3274336283185843e-05, 'epoch': 2.61}
{'loss': 0.7911, 'learning_rate': 1.2831858407079647e-05, 'epoch': 2.62}
{'loss': 0.7817, 'learning_rate': 1.2389380530973452e-05, 'epoch': 2.64}
{'loss': 0.7881, 'learning_rate': 1.1946902654867258e-05, 'epoch': 2.65}
{'loss': 0.791, 'learning_rate': 1.1504424778761062e-05, 'epoch': 2.66}
{'eval_loss': 0.9340135455131531, 'eval_runtime': 190.178, 'eval_samples_per_second': 52.551, 'eval_steps_per_second': 1.646, 'epoch': 2.66}
{'loss': 0.7981, 'learning_rate': 1.1061946902654869e-05, 'epoch': 2.68}
{'loss': 0.8148, 'learning_rate': 1.0619469026548673e-05, 'epoch': 2.69}
{'loss': 0.8131, 'learning_rate': 1.0176991150442479e-05, 'epoch': 2.7}
{'loss': 0.7858, 'learning_rate': 9.734513274336284e-06, 'epoch': 2.71}
{'loss': 0.7778, 'learning_rate': 9.29203539823009e-06, 'epoch': 2.73}
{'loss': 0.7758, 'learning_rate': 8.849557522123894e-06, 'epoch': 2.74}
{'loss': 0.7876, 'learning_rate': 8.407079646017701e-06, 'epoch': 2.75}
{'loss': 0.7755, 'learning_rate': 7.964601769911505e-06, 'epoch': 2.76}
{'loss': 0.8018, 'learning_rate': 7.52212389380531e-06, 'epoch': 2.78}
{'loss': 0.8313, 'learning_rate': 7.079646017699115e-06, 'epoch': 2.79}
{'loss': 0.791, 'learning_rate': 6.6371681415929215e-06, 'epoch': 2.8}
{'loss': 0.7832, 'learning_rate': 6.194690265486726e-06, 'epoch': 2.82}
{'loss': 0.782, 'learning_rate': 5.752212389380531e-06, 'epoch': 2.83}
{'loss': 0.783, 'learning_rate': 5.3097345132743365e-06, 'epoch': 2.84}
{'loss': 0.807, 'learning_rate': 4.867256637168142e-06, 'epoch': 2.85}
{'loss': 0.791, 'learning_rate': 4.424778761061947e-06, 'epoch': 2.87}
{'loss': 0.8041, 'learning_rate': 3.982300884955752e-06, 'epoch': 2.88}
{'loss': 0.8469, 'learning_rate': 3.5398230088495575e-06, 'epoch': 2.89}
{'loss': 0.7777, 'learning_rate': 3.097345132743363e-06, 'epoch': 2.91}
{'loss': 0.7952, 'learning_rate': 2.6548672566371683e-06, 'epoch': 2.92}
{'loss': 0.8069, 'learning_rate': 2.2123893805309734e-06, 'epoch': 2.93}
{'loss': 0.8034, 'learning_rate': 1.7699115044247788e-06, 'epoch': 2.94}
{'loss': 0.8117, 'learning_rate': 1.3274336283185841e-06, 'epoch': 2.96}
{'loss': 0.798, 'learning_rate': 8.849557522123894e-07, 'epoch': 2.97}
{'loss': 0.7767, 'learning_rate': 4.424778761061947e-07, 'epoch': 2.98}
{'loss': 0.795, 'learning_rate': 0.0, 'epoch': 3.0}
{'eval_loss': 0.9346172213554382, 'eval_runtime': 190.1528, 'eval_samples_per_second': 52.558, 'eval_steps_per_second': 1.646, 'epoch': 3.0}
{'train_runtime': 6644.6232, 'train_samples_per_second': 9.03, 'train_steps_per_second': 0.035, 'train_loss': 0.9389365809595484, 'epoch': 3.0}
