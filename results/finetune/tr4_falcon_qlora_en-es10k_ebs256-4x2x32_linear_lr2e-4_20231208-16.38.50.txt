==================================================
FINETUNING PARAMETERS:
base model: tiiuae/falcon-7b
--------------------------------------------------
train_split: [:20000]
dataset_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/en-es_europarl-unpc/europarl-unpc_en-es_bidir.jsonl
validation_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/flores_dev_eng-spa.jsonl
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/flores_dev_spa-eng.jsonl
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/unpc_dev_en-es_unidir.jsonl
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/unpc_dev_es-en_unidir.jsonl
--------------------------------------------------
output_dir: /fs/surtr0/jprats/models/checkpoints/tr4_falcon_qlora_en-es10k_ebs256-4x2x32_linear_lr2e-4_20231208-16.38.50
--------------------------------------------------
learning_rate: 0.0001
lr_scheduler_type: linear
effective batch size: 64
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 32
  CUDA Devices: 2,3,4,5
num_train_epochs: 3
warmup_ratio: 0.03
group_by_length: False
evaluation_strategy: steps
eval_steps: 0.01
--------------------------------------------------
lora_r: 16
lora_alpha: 16
--------------------------------------------------
bf16: True
--------------------------------------------------
use_4bit: True
bnb_4bit_quant_type: nf4
bnb_4bit_compute_dtype: float16
==================================================
==================================================
==================================================FINETUNING PARAMETERS:

FINETUNING PARAMETERS:base model:
base model:  tiiuae/falcon-7btiiuae/falcon-7b

----------------------------------------------------------------------------------------------------

train_split:train_split:  [:20000][:20000]

dataset_files:dataset_files:

	/fs/surtr0/jprats/data/processed/04-finetuning/en-es_europarl-unpc/europarl-unpc_en-es_bidir.jsonl	/fs/surtr0/jprats/data/processed/04-finetuning/en-es_europarl-unpc/europarl-unpc_en-es_bidir.jsonl

validation_files:validation_files:

	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/flores_dev_eng-spa.jsonl	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/flores_dev_eng-spa.jsonl

	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/flores_dev_spa-eng.jsonl	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/flores_dev_spa-eng.jsonl

	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/unpc_dev_en-es_unidir.jsonl	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/unpc_dev_en-es_unidir.jsonl

	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/unpc_dev_es-en_unidir.jsonl	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/unpc_dev_es-en_unidir.jsonl

----------------------------------------------------------------------------------------------------

output_dir:output_dir:  /fs/surtr0/jprats/models/checkpoints/tr4_falcon_qlora_en-es10k_ebs256-4x2x32_linear_lr2e-4_20231208-16.38.50/fs/surtr0/jprats/models/checkpoints/tr4_falcon_qlora_en-es10k_ebs256-4x2x32_linear_lr2e-4_20231208-16.38.50

----------------------------------------------------------------------------------------------------

learning_rate:learning_rate:  0.00010.0001

lr_scheduler_type:lr_scheduler_type:  linearlinear

effective batch size:effective batch size:  6464

  per_device_train_batch_size:  per_device_train_batch_size:  22

  gradient_accumulation_steps:  gradient_accumulation_steps:  3232

  CUDA Devices:  CUDA Devices:  2,3,4,52,3,4,5

num_train_epochs:num_train_epochs:  33

warmup_ratio:warmup_ratio:  0.030.03

group_by_length:group_by_length:  FalseFalse

evaluation_strategy:evaluation_strategy:  stepssteps

eval_steps:eval_steps:  0.010.01

----------------------------------------------------------------------------------------------------

lora_r:lora_r:  1616

lora_alpha:lora_alpha:  1616

----------------------------------------------------------------------------------------------------

bf16:bf16:  TrueTrue

----------------------------------------------------------------------------------------------------

use_4bit:use_4bit:  TrueTrue

bnb_4bit_quant_type:bnb_4bit_quant_type:  nf4nf4

bnb_4bit_compute_dtype:bnb_4bit_compute_dtype:  float16float16

====================================================================================================

==================================================
FINETUNING PARAMETERS:
base model: tiiuae/falcon-7b
--------------------------------------------------
train_split: [:20000]
dataset_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/en-es_europarl-unpc/europarl-unpc_en-es_bidir.jsonl
validation_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/flores_dev_eng-spa.jsonl
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/flores_dev_spa-eng.jsonl
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/unpc_dev_en-es_unidir.jsonl
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/unpc_dev_es-en_unidir.jsonl
--------------------------------------------------
output_dir: /fs/surtr0/jprats/models/checkpoints/tr4_falcon_qlora_en-es10k_ebs256-4x2x32_linear_lr2e-4_20231208-16.38.50
--------------------------------------------------
learning_rate: 0.0001
lr_scheduler_type: linear
effective batch size: 64
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 32
  CUDA Devices: 2,3,4,5
num_train_epochs: 3
warmup_ratio: 0.03
group_by_length: False
evaluation_strategy: steps
eval_steps: 0.01
--------------------------------------------------
lora_r: 16
lora_alpha: 16
--------------------------------------------------
bf16: True
--------------------------------------------------
use_4bit: True
bnb_4bit_quant_type: nf4
bnb_4bit_compute_dtype: float16
==================================================
================================================================================
Your GPU supports bfloat16, you can accelerate training with the argument --bf16
================================================================================
================================================================================
Your GPU supports bfloat16, you can accelerate training with the argument --bf16
================================================================================
================================================================================
Your GPU supports bfloat16, you can accelerate training with the argument --bf16
================================================================================
================================================================================
Your GPU supports bfloat16, you can accelerate training with the argument --bf16
================================================================================
Resulting dataset:
Dataset({
    features: ['text'],
    num_rows: 20000
})
Resulting validation dataset:
Dataset({
    features: ['text'],
    num_rows: 9994
})
Dataset({
    features: ['text'],
    num_rows: 20000
})
False
False
Resulting dataset:
Dataset({
    features: ['text'],
    num_rows: 20000
})
Resulting validation dataset:
Dataset({
    features: ['text'],
    num_rows: 9994
})
Dataset({
    features: ['text'],
    num_rows: 20000
})
False
False
Resulting dataset:
Dataset({
    features: ['text'],
    num_rows: 20000
})
Resulting dataset:
Dataset({
    features: ['text'],
    num_rows: 20000
})
Resulting validation dataset:
Dataset({
    features: ['text'],
    num_rows: 9994
})
Dataset({
    features: ['text'],
    num_rows: 20000
})
False
False
Resulting validation dataset:
Dataset({
    features: ['text'],
    num_rows: 9994
})
Dataset({
    features: ['text'],
    num_rows: 20000
})
False
False
{'loss': 1.022, 'learning_rate': 1.25e-05, 'epoch': 0.01}
{'loss': 1.0183, 'learning_rate': 2.5e-05, 'epoch': 0.03}
{'loss': 1.0086, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.04}
{'eval_loss': 0.9513511061668396, 'eval_runtime': 199.4828, 'eval_samples_per_second': 50.1, 'eval_steps_per_second': 1.569, 'epoch': 0.04}
{'loss': 1.0066, 'learning_rate': 5e-05, 'epoch': 0.05}
{'loss': 0.9932, 'learning_rate': 6.25e-05, 'epoch': 0.06}
{'loss': 1.0012, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.08}
