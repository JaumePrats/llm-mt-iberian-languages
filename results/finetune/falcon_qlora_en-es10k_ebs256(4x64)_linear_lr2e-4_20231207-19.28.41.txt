==================================================
FINETUNING PARAMETERS:
base model: tiiuae/falcon-7b
--------------------------------------------------
train_split: [:20000]
dataset_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/en-es_europarl-unpc/europarl-unpc_en-es_bidir.jsonl
validation_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/flores_dev_eng-spa.jsonl
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/flores_dev_spa-eng.jsonl
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/unpc_dev_en-es_unidir.jsonl
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/unpc_dev_es-en_unidir.jsonl
--------------------------------------------------
output_dir: /fs/surtr0/jprats/models/checkpoints/falcon_qlora_en-es10k_ebs256(4x64)_linear_lr2e-4_20231207-19.28.41
--------------------------------------------------
learning_rate: 0.0001
lr_scheduler_type: linear
effective batch size: 256
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 64
  CUDA Devices: 1
num_train_epochs: 3
warmup_ratio: 0.03
group_by_length: False
evaluation_strategy: steps
eval_steps: 0.01
--------------------------------------------------
lora_r: 16
lora_alpha: 16
--------------------------------------------------
bf16: True
--------------------------------------------------
use_4bit: True
bnb_4bit_quant_type: nf4
bnb_4bit_compute_dtype: float16
==================================================
================================================================================
Your GPU supports bfloat16, you can accelerate training with the argument --bf16
================================================================================
Resulting dataset:
Dataset({
    features: ['text'],
    num_rows: 20000
})
Resulting validation dataset:
Dataset({
    features: ['text'],
    num_rows: 9994
})
Dataset({
    features: ['text'],
    num_rows: 20000
})
False
False
{'loss': 1.0077, 'learning_rate': 1.25e-05, 'epoch': 0.01}
{'loss': 1.0127, 'learning_rate': 2.5e-05, 'epoch': 0.03}
{'loss': 1.0049, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.04}
{'eval_loss': 0.9513392448425293, 'eval_runtime': 714.5169, 'eval_samples_per_second': 13.987, 'eval_steps_per_second': 1.749, 'epoch': 0.04}
{'loss': 1.0183, 'learning_rate': 5e-05, 'epoch': 0.05}
{'loss': 1.007, 'learning_rate': 6.25e-05, 'epoch': 0.06}
{'loss': 0.9719, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.08}
{'eval_loss': 0.9146220088005066, 'eval_runtime': 710.6571, 'eval_samples_per_second': 14.063, 'eval_steps_per_second': 1.759, 'epoch': 0.08}
{'loss': 0.9602, 'learning_rate': 8.75e-05, 'epoch': 0.09}
{'loss': 0.9533, 'learning_rate': 0.0001, 'epoch': 0.1}
{'loss': 0.8822, 'learning_rate': 9.955752212389381e-05, 'epoch': 0.12}
{'eval_loss': 0.8048672080039978, 'eval_runtime': 720.316, 'eval_samples_per_second': 13.874, 'eval_steps_per_second': 1.735, 'epoch': 0.12}
{'loss': 0.8964, 'learning_rate': 9.911504424778762e-05, 'epoch': 0.13}
{'loss': 0.8032, 'learning_rate': 9.867256637168141e-05, 'epoch': 0.14}
{'loss': 0.8356, 'learning_rate': 9.823008849557522e-05, 'epoch': 0.15}
{'eval_loss': 0.7706823348999023, 'eval_runtime': 723.6744, 'eval_samples_per_second': 13.81, 'eval_steps_per_second': 1.727, 'epoch': 0.15}
{'loss': 0.8287, 'learning_rate': 9.778761061946903e-05, 'epoch': 0.17}
{'loss': 0.8443, 'learning_rate': 9.734513274336283e-05, 'epoch': 0.18}
{'loss': 0.8356, 'learning_rate': 9.690265486725664e-05, 'epoch': 0.19}
{'eval_loss': 0.7577359080314636, 'eval_runtime': 717.7967, 'eval_samples_per_second': 13.923, 'eval_steps_per_second': 1.741, 'epoch': 0.19}
{'loss': 0.815, 'learning_rate': 9.646017699115044e-05, 'epoch': 0.2}
{'loss': 0.8094, 'learning_rate': 9.601769911504426e-05, 'epoch': 0.22}
{'loss': 0.7936, 'learning_rate': 9.557522123893806e-05, 'epoch': 0.23}
{'eval_loss': 0.747445821762085, 'eval_runtime': 722.1156, 'eval_samples_per_second': 13.84, 'eval_steps_per_second': 1.731, 'epoch': 0.23}
{'loss': 0.7773, 'learning_rate': 9.513274336283187e-05, 'epoch': 0.24}
{'loss': 0.783, 'learning_rate': 9.469026548672566e-05, 'epoch': 0.26}
{'loss': 0.7794, 'learning_rate': 9.424778761061947e-05, 'epoch': 0.27}
{'eval_loss': 0.7427510023117065, 'eval_runtime': 723.1679, 'eval_samples_per_second': 13.82, 'eval_steps_per_second': 1.729, 'epoch': 0.27}
{'loss': 0.8035, 'learning_rate': 9.380530973451328e-05, 'epoch': 0.28}
{'loss': 0.8086, 'learning_rate': 9.336283185840709e-05, 'epoch': 0.29}
{'loss': 0.7934, 'learning_rate': 9.29203539823009e-05, 'epoch': 0.31}
{'eval_loss': 0.7368705868721008, 'eval_runtime': 710.5046, 'eval_samples_per_second': 14.066, 'eval_steps_per_second': 1.759, 'epoch': 0.31}
{'loss': 0.7647, 'learning_rate': 9.247787610619469e-05, 'epoch': 0.32}
{'loss': 0.7947, 'learning_rate': 9.20353982300885e-05, 'epoch': 0.33}
{'loss': 0.7505, 'learning_rate': 9.15929203539823e-05, 'epoch': 0.35}
{'eval_loss': 0.729736864566803, 'eval_runtime': 712.2446, 'eval_samples_per_second': 14.032, 'eval_steps_per_second': 1.755, 'epoch': 0.35}
{'loss': 0.7894, 'learning_rate': 9.115044247787611e-05, 'epoch': 0.36}
{'loss': 0.7732, 'learning_rate': 9.070796460176992e-05, 'epoch': 0.37}
{'loss': 0.768, 'learning_rate': 9.026548672566371e-05, 'epoch': 0.38}
{'eval_loss': 0.7227508425712585, 'eval_runtime': 717.5573, 'eval_samples_per_second': 13.928, 'eval_steps_per_second': 1.742, 'epoch': 0.38}
{'loss': 0.7447, 'learning_rate': 8.982300884955752e-05, 'epoch': 0.4}
{'loss': 0.7729, 'learning_rate': 8.938053097345133e-05, 'epoch': 0.41}
{'loss': 0.7275, 'learning_rate': 8.893805309734515e-05, 'epoch': 0.42}
{'eval_loss': 0.7176728248596191, 'eval_runtime': 714.3279, 'eval_samples_per_second': 13.991, 'eval_steps_per_second': 1.75, 'epoch': 0.42}
{'loss': 0.7805, 'learning_rate': 8.849557522123895e-05, 'epoch': 0.44}
{'loss': 0.7239, 'learning_rate': 8.805309734513275e-05, 'epoch': 0.45}
{'loss': 0.7507, 'learning_rate': 8.761061946902655e-05, 'epoch': 0.46}
