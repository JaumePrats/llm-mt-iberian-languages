==================================================
FINETUNING PARAMETERS:
base model: tiiuae/falcon-7b
resume_from_checkpoint: /fs/surtr0/jprats/models/checkpoints/falcon_qlora_en-es100k_ebs256_linear_lr2e-4_20231207-13.56.08/checkpoint-168
--------------------------------------------------
train_split: [:200000]
dataset_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/en-es_europarl-unpc/europarl-unpc_en-es_bidir.jsonl
validation_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/flores_dev_eng-spa.jsonl
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/flores_dev_spa-eng.jsonl
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/unpc_dev_en-es_unidir.jsonl
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/unpc_dev_es-en_unidir.jsonl
--------------------------------------------------
output_dir: /fs/surtr0/jprats/models/checkpoints/falcon_qlora_en-es100k_ebs256_linear_lr2e-4_RESUMED_20231207-19.12.37
--------------------------------------------------
learning_rate: 0.0001
lr_scheduler_type: linear
effective batch size: 256
  per_device_train_batch_size: 16
  gradient_accumulation_steps: 16
  CUDA Devices: 2
num_train_epochs: 3
warmup_ratio: 0.03
group_by_length: False
evaluation_strategy: steps
eval_steps: 0.01
--------------------------------------------------
lora_r: 16
lora_alpha: 16
--------------------------------------------------
bf16: True
--------------------------------------------------
use_4bit: True
bnb_4bit_quant_type: nf4
bnb_4bit_compute_dtype: float16
==================================================
================================================================================
Your GPU supports bfloat16, you can accelerate training with the argument --bf16
================================================================================
Resulting dataset:
Dataset({
    features: ['text'],
    num_rows: 200000
})
Resulting validation dataset:
Dataset({
    features: ['text'],
    num_rows: 9994
})
Dataset({
    features: ['text'],
    num_rows: 200000
})
False
False
{'loss': 0.6977, 'learning_rate': 9.568661971830986e-05, 'epoch': 0.22}
{'loss': 0.6858, 'learning_rate': 9.564260563380282e-05, 'epoch': 0.22}
{'loss': 0.6798, 'learning_rate': 9.559859154929578e-05, 'epoch': 0.22}
{'loss': 0.6698, 'learning_rate': 9.555457746478874e-05, 'epoch': 0.22}
{'loss': 0.6997, 'learning_rate': 9.551056338028169e-05, 'epoch': 0.22}
{'loss': 0.6462, 'learning_rate': 9.546654929577465e-05, 'epoch': 0.22}
{'loss': 0.6589, 'learning_rate': 9.542253521126761e-05, 'epoch': 0.22}
{'loss': 0.6544, 'learning_rate': 9.537852112676057e-05, 'epoch': 0.23}
{'loss': 0.6738, 'learning_rate': 9.533450704225353e-05, 'epoch': 0.23}
{'loss': 0.6882, 'learning_rate': 9.529049295774649e-05, 'epoch': 0.23}
{'loss': 0.6958, 'learning_rate': 9.524647887323945e-05, 'epoch': 0.23}
{'loss': 0.6331, 'learning_rate': 9.52024647887324e-05, 'epoch': 0.23}
{'loss': 0.6282, 'learning_rate': 9.515845070422535e-05, 'epoch': 0.23}
{'loss': 0.7113, 'learning_rate': 9.511443661971831e-05, 'epoch': 0.23}
{'loss': 0.6358, 'learning_rate': 9.507042253521127e-05, 'epoch': 0.23}
{'loss': 0.6523, 'learning_rate': 9.502640845070423e-05, 'epoch': 0.24}
{'loss': 0.6696, 'learning_rate': 9.498239436619719e-05, 'epoch': 0.24}
{'loss': 0.6999, 'learning_rate': 9.493838028169015e-05, 'epoch': 0.24}
{'loss': 0.6712, 'learning_rate': 9.489436619718311e-05, 'epoch': 0.24}
