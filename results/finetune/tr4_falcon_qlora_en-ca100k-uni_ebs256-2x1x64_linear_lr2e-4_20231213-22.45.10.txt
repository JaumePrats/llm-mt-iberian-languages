==================================================
FINETUNING PARAMETERS:
base model: tiiuae/falcon-7b
--------------------------------------------------
train_split: [:100000]
dataset_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/ca-en_multimacocu/multimacocu_en-ca_unidir.jsonl
validation_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/multimacocu_en-ca_unidir.jsonl
--------------------------------------------------
output_dir: /fs/surtr0/jprats/models/checkpoints/tr4_falcon_qlora_en-ca100k-uni_ebs256-2x1x64_linear_lr2e-4_20231213-22.45.10
--------------------------------------------------
learning_rate: 0.0001
lr_scheduler_type: linear
effective batch size: 64
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 64
  CUDA Devices: 2,3,4,5
num_train_epochs: 3
warmup_ratio: 0.03
group_by_length: False
evaluation_strategy: steps
eval_steps: 0.11111
--------------------------------------------------
lora_r: 16
lora_alpha: 16
--------------------------------------------------
bf16: True
--------------------------------------------------
use_4bit: True
bnb_4bit_quant_type: nf4
bnb_4bit_compute_dtype: float16
==================================================
==================================================
FINETUNING PARAMETERS:
base model: tiiuae/falcon-7b
--------------------------------------------------
train_split: [:100000]
dataset_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/ca-en_multimacocu/multimacocu_en-ca_unidir.jsonl
validation_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/multimacocu_en-ca_unidir.jsonl
--------------------------------------------------
output_dir: /fs/surtr0/jprats/models/checkpoints/tr4_falcon_qlora_en-ca100k-uni_ebs256-2x1x64_linear_lr2e-4_20231213-22.45.10
--------------------------------------------------
learning_rate: 0.0001
lr_scheduler_type: linear
effective batch size: 64
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 64
  CUDA Devices: 2,3,4,5
num_train_epochs: 3
warmup_ratio: 0.03
group_by_length: False
evaluation_strategy: steps
eval_steps: 0.11111
--------------------------------------------------
lora_r: 16
lora_alpha: 16
--------------------------------------------------
bf16: True
--------------------------------------------------
use_4bit: True
bnb_4bit_quant_type: nf4
bnb_4bit_compute_dtype: float16
==================================================
==================================================
FINETUNING PARAMETERS:
base model: tiiuae/falcon-7b
--------------------------------------------------
train_split: [:100000]
dataset_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/ca-en_multimacocu/multimacocu_en-ca_unidir.jsonl
validation_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/multimacocu_en-ca_unidir.jsonl
--------------------------------------------------
output_dir: /fs/surtr0/jprats/models/checkpoints/tr4_falcon_qlora_en-ca100k-uni_ebs256-2x1x64_linear_lr2e-4_20231213-22.45.10
--------------------------------------------------
learning_rate: 0.0001
lr_scheduler_type: linear
effective batch size: 64
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 64
  CUDA Devices: 2,3,4,5
num_train_epochs: 3
warmup_ratio: 0.03
group_by_length: False
evaluation_strategy: steps
eval_steps: 0.11111
--------------------------------------------------
lora_r: 16
lora_alpha: 16
--------------------------------------------------
bf16: True
--------------------------------------------------
use_4bit: True
bnb_4bit_quant_type: nf4
bnb_4bit_compute_dtype: float16
==================================================
==================================================
FINETUNING PARAMETERS:
base model: tiiuae/falcon-7b
--------------------------------------------------
train_split: [:100000]
dataset_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/ca-en_multimacocu/multimacocu_en-ca_unidir.jsonl
validation_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/multimacocu_en-ca_unidir.jsonl
--------------------------------------------------
output_dir: /fs/surtr0/jprats/models/checkpoints/tr4_falcon_qlora_en-ca100k-uni_ebs256-2x1x64_linear_lr2e-4_20231213-22.45.10
--------------------------------------------------
learning_rate: 0.0001
lr_scheduler_type: linear
effective batch size: 64
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 64
  CUDA Devices: 2,3,4,5
num_train_epochs: 3
warmup_ratio: 0.03
group_by_length: False
evaluation_strategy: steps
eval_steps: 0.11111
--------------------------------------------------
lora_r: 16
lora_alpha: 16
--------------------------------------------------
bf16: True
--------------------------------------------------
use_4bit: True
bnb_4bit_quant_type: nf4
bnb_4bit_compute_dtype: float16
==================================================
================================================================================
Your GPU supports bfloat16, you can accelerate training with the argument --bf16
================================================================================
================================================================================
Your GPU supports bfloat16, you can accelerate training with the argument --bf16
================================================================================
================================================================================
Your GPU supports bfloat16, you can accelerate training with the argument --bf16
================================================================================
================================================================================
Your GPU supports bfloat16, you can accelerate training with the argument --bf16
================================================================================
Resulting dataset:
Dataset({
    features: ['text'],
    num_rows: 100000
})
Resulting validation dataset:
Dataset({
    features: ['text'],
    num_rows: 5000
})
Dataset({
    features: ['text'],
    num_rows: 100000
})
False
False
Resulting dataset:
Dataset({
    features: ['text'],
    num_rows: 100000
})
Resulting validation dataset:
Dataset({
    features: ['text'],
    num_rows: 5000
})
Dataset({
    features: ['text'],
    num_rows: 100000
})
False
False
Resulting dataset:
Dataset({
    features: ['text'],
    num_rows: 100000
})
Resulting validation dataset:
Dataset({
    features: ['text'],
    num_rows: 5000
})
Dataset({
    features: ['text'],
    num_rows: 100000
})
False
False
Resulting dataset:
Dataset({
    features: ['text'],
    num_rows: 100000
})
Resulting validation dataset:
Dataset({
    features: ['text'],
    num_rows: 5000
})
Dataset({
    features: ['text'],
    num_rows: 100000
})
False
False
{'loss': 1.6669, 'learning_rate': 2.777777777777778e-06, 'epoch': 0.0}
{'loss': 1.6365, 'learning_rate': 5.555555555555556e-06, 'epoch': 0.01}
{'loss': 1.6346, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.01}
{'loss': 1.6326, 'learning_rate': 1.1111111111111112e-05, 'epoch': 0.01}
{'loss': 1.7243, 'learning_rate': 1.388888888888889e-05, 'epoch': 0.01}
{'loss': 1.7411, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.02}
{'loss': 1.6976, 'learning_rate': 1.9444444444444445e-05, 'epoch': 0.02}
{'loss': 1.6798, 'learning_rate': 2.2222222222222223e-05, 'epoch': 0.02}
{'loss': 1.6373, 'learning_rate': 2.5e-05, 'epoch': 0.02}
{'loss': 1.6562, 'learning_rate': 2.777777777777778e-05, 'epoch': 0.03}
{'loss': 1.5303, 'learning_rate': 3.055555555555556e-05, 'epoch': 0.03}
{'loss': 1.6324, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.03}
{'loss': 1.6022, 'learning_rate': 3.611111111111111e-05, 'epoch': 0.03}
{'loss': 1.5944, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.04}
{'loss': 1.537, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.04}
{'loss': 1.537, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.04}
{'loss': 1.4692, 'learning_rate': 4.722222222222222e-05, 'epoch': 0.04}
{'loss': 1.436, 'learning_rate': 5e-05, 'epoch': 0.05}
{'loss': 1.4547, 'learning_rate': 5.2777777777777784e-05, 'epoch': 0.05}
{'loss': 1.4747, 'learning_rate': 5.555555555555556e-05, 'epoch': 0.05}
{'loss': 1.4351, 'learning_rate': 5.833333333333334e-05, 'epoch': 0.05}
{'loss': 1.3383, 'learning_rate': 6.111111111111112e-05, 'epoch': 0.06}
{'loss': 1.3863, 'learning_rate': 6.388888888888888e-05, 'epoch': 0.06}
{'loss': 1.3534, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.06}
{'loss': 1.3331, 'learning_rate': 6.944444444444444e-05, 'epoch': 0.06}
{'loss': 1.375, 'learning_rate': 7.222222222222222e-05, 'epoch': 0.07}
