==================================================
FINETUNING PARAMETERS:
base model: tiiuae/falcon-7b
--------------------------------------------------
train_split: [:10000]
dataset_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/ca-en_multimacocu/multimacocu_en-ca_unidir.jsonl
validation_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/multimacocu_en-ca_unidir.jsonl
--------------------------------------------------
output_dir: /fs/surtr0/jprats/models/checkpoints/tr4_falcon_qlora_en-ca10k-uni_ebs256-4x1x64_linear_lr2e-4_20231214-12.39.06
--------------------------------------------------
learning_rate: 0.0001
lr_scheduler_type: linear
effective batch size: 64
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 64
  CUDA Devices: 2,3,4,5
num_train_epochs: 3
warmup_ratio: 0.03
group_by_length: False
evaluation_strategy: steps
eval_steps: 0.11111
--------------------------------------------------
lora_r: 16
lora_alpha: 16
--------------------------------------------------
bf16: True
--------------------------------------------------
use_4bit: True
bnb_4bit_quant_type: nf4
bnb_4bit_compute_dtype: float16
==================================================
==================================================
FINETUNING PARAMETERS:
base model: tiiuae/falcon-7b
--------------------------------------------------
train_split: [:10000]
dataset_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/ca-en_multimacocu/multimacocu_en-ca_unidir.jsonl
validation_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/multimacocu_en-ca_unidir.jsonl
--------------------------------------------------
output_dir: /fs/surtr0/jprats/models/checkpoints/tr4_falcon_qlora_en-ca10k-uni_ebs256-4x1x64_linear_lr2e-4_20231214-12.39.06
--------------------------------------------------
learning_rate: 0.0001
lr_scheduler_type: linear
effective batch size: 64
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 64
  CUDA Devices: 2,3,4,5
num_train_epochs: 3
warmup_ratio: 0.03
group_by_length: False
evaluation_strategy: steps
eval_steps: 0.11111
--------------------------------------------------
lora_r: 16
lora_alpha: 16
--------------------------------------------------
bf16: True
--------------------------------------------------
use_4bit: True
bnb_4bit_quant_type: nf4
bnb_4bit_compute_dtype: float16
==================================================
==================================================
FINETUNING PARAMETERS:
base model: tiiuae/falcon-7b
--------------------------------------------------
train_split: [:10000]
dataset_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/ca-en_multimacocu/multimacocu_en-ca_unidir.jsonl
validation_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/multimacocu_en-ca_unidir.jsonl
--------------------------------------------------
output_dir: /fs/surtr0/jprats/models/checkpoints/tr4_falcon_qlora_en-ca10k-uni_ebs256-4x1x64_linear_lr2e-4_20231214-12.39.06
--------------------------------------------------
learning_rate: 0.0001
lr_scheduler_type: linear
effective batch size: 64
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 64
  CUDA Devices: 2,3,4,5
num_train_epochs: 3
warmup_ratio: 0.03
group_by_length: False
evaluation_strategy: steps
eval_steps: 0.11111
--------------------------------------------------
lora_r: 16
lora_alpha: 16
--------------------------------------------------
bf16: True
--------------------------------------------------
use_4bit: True
bnb_4bit_quant_type: nf4
bnb_4bit_compute_dtype: float16
==================================================
==================================================
FINETUNING PARAMETERS:
base model: tiiuae/falcon-7b
--------------------------------------------------
train_split: [:10000]
dataset_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/ca-en_multimacocu/multimacocu_en-ca_unidir.jsonl
validation_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/multimacocu_en-ca_unidir.jsonl
--------------------------------------------------
output_dir: /fs/surtr0/jprats/models/checkpoints/tr4_falcon_qlora_en-ca10k-uni_ebs256-4x1x64_linear_lr2e-4_20231214-12.39.06
--------------------------------------------------
learning_rate: 0.0001
lr_scheduler_type: linear
effective batch size: 64
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 64
  CUDA Devices: 2,3,4,5
num_train_epochs: 3
warmup_ratio: 0.03
group_by_length: False
evaluation_strategy: steps
eval_steps: 0.11111
--------------------------------------------------
lora_r: 16
lora_alpha: 16
--------------------------------------------------
bf16: True
--------------------------------------------------
use_4bit: True
bnb_4bit_quant_type: nf4
bnb_4bit_compute_dtype: float16
==================================================
================================================================================
Your GPU supports bfloat16, you can accelerate training with the argument --bf16
================================================================================
================================================================================
Your GPU supports bfloat16, you can accelerate training with the argument --bf16
================================================================================
================================================================================
Your GPU supports bfloat16, you can accelerate training with the argument --bf16
================================================================================
================================================================================
Your GPU supports bfloat16, you can accelerate training with the argument --bf16
================================================================================
Resulting dataset:
Dataset({
    features: ['text'],
    num_rows: 10000
})
Resulting dataset:
Dataset({
    features: ['text'],
    num_rows: 10000
})
Resulting validation dataset:
Dataset({
    features: ['text'],
    num_rows: 5000
})
Dataset({
    features: ['text'],
    num_rows: 10000
})
False
False
Resulting validation dataset:
Dataset({
    features: ['text'],
    num_rows: 5000
})
Dataset({
    features: ['text'],
    num_rows: 10000
})
False
False
Resulting dataset:
Dataset({
    features: ['text'],
    num_rows: 10000
})
Resulting dataset:
Dataset({
    features: ['text'],
    num_rows: 10000
})
Resulting validation dataset:
Dataset({
    features: ['text'],
    num_rows: 5000
})
Dataset({
    features: ['text'],
    num_rows: 10000
})
False
False
Resulting validation dataset:
Dataset({
    features: ['text'],
    num_rows: 5000
})
Dataset({
    features: ['text'],
    num_rows: 10000
})
False
False
{'loss': 1.6992, 'learning_rate': 2.5e-05, 'epoch': 0.03}
{'loss': 1.6046, 'learning_rate': 5e-05, 'epoch': 0.05}
{'loss': 1.7057, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.08}
{'loss': 1.6565, 'learning_rate': 0.0001, 'epoch': 0.1}
{'loss': 1.5757, 'learning_rate': 9.911504424778762e-05, 'epoch': 0.13}
{'loss': 1.5661, 'learning_rate': 9.823008849557522e-05, 'epoch': 0.15}
{'loss': 1.5247, 'learning_rate': 9.734513274336283e-05, 'epoch': 0.18}
{'loss': 1.4157, 'learning_rate': 9.646017699115044e-05, 'epoch': 0.2}
{'loss': 1.4275, 'learning_rate': 9.557522123893806e-05, 'epoch': 0.23}
{'loss': 1.3449, 'learning_rate': 9.469026548672566e-05, 'epoch': 0.26}
{'loss': 1.3815, 'learning_rate': 9.380530973451328e-05, 'epoch': 0.28}
{'loss': 1.3759, 'learning_rate': 9.29203539823009e-05, 'epoch': 0.31}
{'loss': 1.323, 'learning_rate': 9.20353982300885e-05, 'epoch': 0.33}
{'eval_loss': 1.4684380292892456, 'eval_runtime': 104.4688, 'eval_samples_per_second': 47.861, 'eval_steps_per_second': 1.503, 'epoch': 0.33}
{'loss': 1.3874, 'learning_rate': 9.115044247787611e-05, 'epoch': 0.36}
{'loss': 1.3121, 'learning_rate': 9.026548672566371e-05, 'epoch': 0.38}
{'loss': 1.4188, 'learning_rate': 8.938053097345133e-05, 'epoch': 0.41}
{'loss': 1.3927, 'learning_rate': 8.849557522123895e-05, 'epoch': 0.44}
{'loss': 1.3547, 'learning_rate': 8.761061946902655e-05, 'epoch': 0.46}
{'loss': 1.304, 'learning_rate': 8.672566371681417e-05, 'epoch': 0.49}
{'loss': 1.3158, 'learning_rate': 8.584070796460177e-05, 'epoch': 0.51}
{'loss': 1.3558, 'learning_rate': 8.495575221238938e-05, 'epoch': 0.54}
{'loss': 1.3247, 'learning_rate': 8.4070796460177e-05, 'epoch': 0.56}
{'loss': 1.3631, 'learning_rate': 8.31858407079646e-05, 'epoch': 0.59}
{'loss': 1.3279, 'learning_rate': 8.230088495575221e-05, 'epoch': 0.61}
{'loss': 1.3285, 'learning_rate': 8.141592920353983e-05, 'epoch': 0.64}
{'loss': 1.366, 'learning_rate': 8.053097345132744e-05, 'epoch': 0.67}
{'eval_loss': 1.3919402360916138, 'eval_runtime': 104.5437, 'eval_samples_per_second': 47.827, 'eval_steps_per_second': 1.502, 'epoch': 0.67}
{'loss': 1.3084, 'learning_rate': 7.964601769911504e-05, 'epoch': 0.69}
{'loss': 1.2883, 'learning_rate': 7.876106194690266e-05, 'epoch': 0.72}
{'loss': 1.3335, 'learning_rate': 7.787610619469027e-05, 'epoch': 0.74}
{'loss': 1.2865, 'learning_rate': 7.699115044247787e-05, 'epoch': 0.77}
{'loss': 1.3617, 'learning_rate': 7.610619469026549e-05, 'epoch': 0.79}
{'loss': 1.2123, 'learning_rate': 7.522123893805309e-05, 'epoch': 0.82}
{'loss': 1.2311, 'learning_rate': 7.433628318584072e-05, 'epoch': 0.84}
{'loss': 1.2834, 'learning_rate': 7.345132743362832e-05, 'epoch': 0.87}
{'loss': 1.288, 'learning_rate': 7.256637168141593e-05, 'epoch': 0.9}
{'loss': 1.2512, 'learning_rate': 7.168141592920355e-05, 'epoch': 0.92}
{'loss': 1.255, 'learning_rate': 7.079646017699115e-05, 'epoch': 0.95}
{'loss': 1.3353, 'learning_rate': 6.991150442477876e-05, 'epoch': 0.97}
{'loss': 1.2974, 'learning_rate': 6.902654867256638e-05, 'epoch': 1.0}
{'eval_loss': 1.3406673669815063, 'eval_runtime': 104.5368, 'eval_samples_per_second': 47.83, 'eval_steps_per_second': 1.502, 'epoch': 1.0}
{'loss': 1.2823, 'learning_rate': 6.814159292035398e-05, 'epoch': 1.02}
{'loss': 1.1965, 'learning_rate': 6.725663716814161e-05, 'epoch': 1.05}
{'loss': 1.2444, 'learning_rate': 6.637168141592921e-05, 'epoch': 1.08}
{'loss': 1.2497, 'learning_rate': 6.548672566371682e-05, 'epoch': 1.1}
{'loss': 1.1875, 'learning_rate': 6.460176991150442e-05, 'epoch': 1.13}
{'loss': 1.2188, 'learning_rate': 6.371681415929204e-05, 'epoch': 1.15}
{'loss': 1.2194, 'learning_rate': 6.283185840707965e-05, 'epoch': 1.18}
{'loss': 1.1998, 'learning_rate': 6.194690265486725e-05, 'epoch': 1.2}
{'loss': 1.2013, 'learning_rate': 6.106194690265487e-05, 'epoch': 1.23}
{'loss': 1.1763, 'learning_rate': 6.017699115044248e-05, 'epoch': 1.25}
{'loss': 1.172, 'learning_rate': 5.92920353982301e-05, 'epoch': 1.28}
{'loss': 1.1865, 'learning_rate': 5.8407079646017705e-05, 'epoch': 1.31}
{'loss': 1.1281, 'learning_rate': 5.752212389380531e-05, 'epoch': 1.33}
{'eval_loss': 1.2998263835906982, 'eval_runtime': 104.5554, 'eval_samples_per_second': 47.822, 'eval_steps_per_second': 1.502, 'epoch': 1.33}
{'loss': 1.2013, 'learning_rate': 5.663716814159292e-05, 'epoch': 1.36}
{'loss': 1.1259, 'learning_rate': 5.575221238938053e-05, 'epoch': 1.38}
{'loss': 1.2411, 'learning_rate': 5.486725663716814e-05, 'epoch': 1.41}
{'loss': 1.2195, 'learning_rate': 5.398230088495575e-05, 'epoch': 1.43}
{'loss': 1.1935, 'learning_rate': 5.309734513274337e-05, 'epoch': 1.46}
{'loss': 1.1337, 'learning_rate': 5.221238938053098e-05, 'epoch': 1.48}
{'loss': 1.1513, 'learning_rate': 5.132743362831859e-05, 'epoch': 1.51}
{'loss': 1.1924, 'learning_rate': 5.0442477876106195e-05, 'epoch': 1.54}
{'loss': 1.1636, 'learning_rate': 4.955752212389381e-05, 'epoch': 1.56}
{'loss': 1.2274, 'learning_rate': 4.867256637168142e-05, 'epoch': 1.59}
{'loss': 1.1698, 'learning_rate': 4.778761061946903e-05, 'epoch': 1.61}
{'loss': 1.1658, 'learning_rate': 4.690265486725664e-05, 'epoch': 1.64}
{'loss': 1.1979, 'learning_rate': 4.601769911504425e-05, 'epoch': 1.66}
{'eval_loss': 1.2670949697494507, 'eval_runtime': 104.5101, 'eval_samples_per_second': 47.842, 'eval_steps_per_second': 1.502, 'epoch': 1.66}
{'loss': 1.163, 'learning_rate': 4.5132743362831855e-05, 'epoch': 1.69}
{'loss': 1.148, 'learning_rate': 4.4247787610619477e-05, 'epoch': 1.72}
{'loss': 1.1843, 'learning_rate': 4.3362831858407084e-05, 'epoch': 1.74}
{'loss': 1.1325, 'learning_rate': 4.247787610619469e-05, 'epoch': 1.77}
{'loss': 1.2138, 'learning_rate': 4.15929203539823e-05, 'epoch': 1.79}
{'loss': 1.0794, 'learning_rate': 4.0707964601769914e-05, 'epoch': 1.82}
{'loss': 1.078, 'learning_rate': 3.982300884955752e-05, 'epoch': 1.84}
{'loss': 1.1454, 'learning_rate': 3.893805309734514e-05, 'epoch': 1.87}
{'loss': 1.1539, 'learning_rate': 3.8053097345132744e-05, 'epoch': 1.89}
{'loss': 1.1215, 'learning_rate': 3.716814159292036e-05, 'epoch': 1.92}
{'loss': 1.1089, 'learning_rate': 3.628318584070797e-05, 'epoch': 1.95}
{'loss': 1.1983, 'learning_rate': 3.5398230088495574e-05, 'epoch': 1.97}
{'loss': 1.1796, 'learning_rate': 3.451327433628319e-05, 'epoch': 2.0}
{'eval_loss': 1.2440539598464966, 'eval_runtime': 104.4079, 'eval_samples_per_second': 47.889, 'eval_steps_per_second': 1.504, 'epoch': 2.0}
{'loss': 1.143, 'learning_rate': 3.3628318584070804e-05, 'epoch': 2.02}
{'loss': 1.0978, 'learning_rate': 3.274336283185841e-05, 'epoch': 2.05}
{'loss': 1.1155, 'learning_rate': 3.185840707964602e-05, 'epoch': 2.07}
{'loss': 1.1361, 'learning_rate': 3.097345132743363e-05, 'epoch': 2.1}
{'loss': 1.0651, 'learning_rate': 3.008849557522124e-05, 'epoch': 2.12}
{'loss': 1.0969, 'learning_rate': 2.9203539823008852e-05, 'epoch': 2.15}
{'loss': 1.1037, 'learning_rate': 2.831858407079646e-05, 'epoch': 2.18}
{'loss': 1.0988, 'learning_rate': 2.743362831858407e-05, 'epoch': 2.2}
{'loss': 1.0732, 'learning_rate': 2.6548672566371686e-05, 'epoch': 2.23}
{'loss': 1.0671, 'learning_rate': 2.5663716814159294e-05, 'epoch': 2.25}
{'loss': 1.0522, 'learning_rate': 2.4778761061946905e-05, 'epoch': 2.28}
{'loss': 1.1005, 'learning_rate': 2.3893805309734516e-05, 'epoch': 2.3}
{'loss': 1.0472, 'learning_rate': 2.3008849557522124e-05, 'epoch': 2.33}
{'eval_loss': 1.2308459281921387, 'eval_runtime': 104.5936, 'eval_samples_per_second': 47.804, 'eval_steps_per_second': 1.501, 'epoch': 2.33}
{'loss': 1.0883, 'learning_rate': 2.2123893805309738e-05, 'epoch': 2.36}
{'loss': 1.0376, 'learning_rate': 2.1238938053097346e-05, 'epoch': 2.38}
{'loss': 1.1404, 'learning_rate': 2.0353982300884957e-05, 'epoch': 2.41}
{'loss': 1.1151, 'learning_rate': 1.946902654867257e-05, 'epoch': 2.43}
{'loss': 1.117, 'learning_rate': 1.858407079646018e-05, 'epoch': 2.46}
{'loss': 1.0481, 'learning_rate': 1.7699115044247787e-05, 'epoch': 2.48}
{'loss': 1.0507, 'learning_rate': 1.6814159292035402e-05, 'epoch': 2.51}
{'loss': 1.1052, 'learning_rate': 1.592920353982301e-05, 'epoch': 2.53}
{'loss': 1.0849, 'learning_rate': 1.504424778761062e-05, 'epoch': 2.56}
{'loss': 1.1533, 'learning_rate': 1.415929203539823e-05, 'epoch': 2.59}
{'loss': 1.1007, 'learning_rate': 1.3274336283185843e-05, 'epoch': 2.61}
{'loss': 1.1018, 'learning_rate': 1.2389380530973452e-05, 'epoch': 2.64}
{'loss': 1.1127, 'learning_rate': 1.1504424778761062e-05, 'epoch': 2.66}
{'eval_loss': 1.223378300666809, 'eval_runtime': 104.6074, 'eval_samples_per_second': 47.798, 'eval_steps_per_second': 1.501, 'epoch': 2.66}
{'loss': 1.0976, 'learning_rate': 1.0619469026548673e-05, 'epoch': 2.69}
{'loss': 1.0795, 'learning_rate': 9.734513274336284e-06, 'epoch': 2.71}
{'loss': 1.1188, 'learning_rate': 8.849557522123894e-06, 'epoch': 2.74}
{'loss': 1.0976, 'learning_rate': 7.964601769911505e-06, 'epoch': 2.76}
{'loss': 1.1278, 'learning_rate': 7.079646017699115e-06, 'epoch': 2.79}
{'loss': 1.0189, 'learning_rate': 6.194690265486726e-06, 'epoch': 2.82}
{'loss': 1.0344, 'learning_rate': 5.3097345132743365e-06, 'epoch': 2.84}
{'loss': 1.068, 'learning_rate': 4.424778761061947e-06, 'epoch': 2.87}
{'loss': 1.1146, 'learning_rate': 3.5398230088495575e-06, 'epoch': 2.89}
{'loss': 1.0649, 'learning_rate': 2.6548672566371683e-06, 'epoch': 2.92}
{'loss': 1.0614, 'learning_rate': 1.7699115044247788e-06, 'epoch': 2.94}
{'loss': 1.1582, 'learning_rate': 8.849557522123894e-07, 'epoch': 2.97}
{'loss': 1.1296, 'learning_rate': 0.0, 'epoch': 3.0}
{'eval_loss': 1.2216894626617432, 'eval_runtime': 104.6293, 'eval_samples_per_second': 47.788, 'eval_steps_per_second': 1.501, 'epoch': 3.0}
{'train_runtime': 3381.2543, 'train_samples_per_second': 8.872, 'train_steps_per_second': 0.035, 'train_loss': 1.2176170858562503, 'epoch': 3.0}
