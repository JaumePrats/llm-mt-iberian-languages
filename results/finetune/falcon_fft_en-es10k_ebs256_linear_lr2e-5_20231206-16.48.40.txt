==================================================
FINETUNING PARAMETERS:
base model: tiiuae/falcon-7b
--------------------------------------------------
train_split: [:20000]
dataset_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/en-es_europarl-unpc/europarl-unpc_en-es_bidir.jsonl
validation_files:
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/flores_dev_eng-spa.jsonl
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/flores_dev_spa-eng.jsonl
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/unpc_dev_en-es_unidir.jsonl
	/fs/surtr0/jprats/data/processed/04-finetuning/devsets/unpc_dev_es-en_unidir.jsonl
--------------------------------------------------
output_dir: /fs/surtr0/jprats/models/checkpoints/falcon_fft_en-es10k_ebs256_linear_lr2e-5_20231206-16.48.40
--------------------------------------------------
learning_rate: 2e-05
lr_scheduler_type: linear
effective batch size: 256
  per_device_train_batch_size: 16
  gradient_accumulation_steps: 16
  CUDA Devices: 3,4,5,6
eval_steps: 0.01
warmup_ratio: 0.03
group_by_length: False
evaluation_strategy: steps
eval_steps: 0.01
--------------------------------------------------
bf16: True
==================================================
Resulting dataset:
Dataset({
    features: ['text'],
    num_rows: 20000
})
Resulting validation dataset:
Dataset({
    features: ['text'],
    num_rows: 9994
})
Dataset({
    features: ['text'],
    num_rows: 20000
})
False
False
Grad req: transformer.word_embeddings.weight
Grad req: transformer.h.0.self_attention.query_key_value.weight
Grad req: transformer.h.0.self_attention.dense.weight
Grad req: transformer.h.0.mlp.dense_h_to_4h.weight
Grad req: transformer.h.0.mlp.dense_4h_to_h.weight
Grad req: transformer.h.0.input_layernorm.weight
Grad req: transformer.h.0.input_layernorm.bias
Grad req: transformer.h.1.self_attention.query_key_value.weight
Grad req: transformer.h.1.self_attention.dense.weight
Grad req: transformer.h.1.mlp.dense_h_to_4h.weight
Grad req: transformer.h.1.mlp.dense_4h_to_h.weight
Grad req: transformer.h.1.input_layernorm.weight
Grad req: transformer.h.1.input_layernorm.bias
Grad req: transformer.h.2.self_attention.query_key_value.weight
Grad req: transformer.h.2.self_attention.dense.weight
Grad req: transformer.h.2.mlp.dense_h_to_4h.weight
Grad req: transformer.h.2.mlp.dense_4h_to_h.weight
Grad req: transformer.h.2.input_layernorm.weight
Grad req: transformer.h.2.input_layernorm.bias
Grad req: transformer.h.3.self_attention.query_key_value.weight
Grad req: transformer.h.3.self_attention.dense.weight
Grad req: transformer.h.3.mlp.dense_h_to_4h.weight
Grad req: transformer.h.3.mlp.dense_4h_to_h.weight
Grad req: transformer.h.3.input_layernorm.weight
Grad req: transformer.h.3.input_layernorm.bias
Grad req: transformer.h.4.self_attention.query_key_value.weight
Grad req: transformer.h.4.self_attention.dense.weight
Grad req: transformer.h.4.mlp.dense_h_to_4h.weight
Grad req: transformer.h.4.mlp.dense_4h_to_h.weight
Grad req: transformer.h.4.input_layernorm.weight
Grad req: transformer.h.4.input_layernorm.bias
Grad req: transformer.h.5.self_attention.query_key_value.weight
Grad req: transformer.h.5.self_attention.dense.weight
Grad req: transformer.h.5.mlp.dense_h_to_4h.weight
Grad req: transformer.h.5.mlp.dense_4h_to_h.weight
Grad req: transformer.h.5.input_layernorm.weight
Grad req: transformer.h.5.input_layernorm.bias
Grad req: transformer.h.6.self_attention.query_key_value.weight
Grad req: transformer.h.6.self_attention.dense.weight
Grad req: transformer.h.6.mlp.dense_h_to_4h.weight
Grad req: transformer.h.6.mlp.dense_4h_to_h.weight
Grad req: transformer.h.6.input_layernorm.weight
Grad req: transformer.h.6.input_layernorm.bias
Grad req: transformer.h.7.self_attention.query_key_value.weight
Grad req: transformer.h.7.self_attention.dense.weight
Grad req: transformer.h.7.mlp.dense_h_to_4h.weight
Grad req: transformer.h.7.mlp.dense_4h_to_h.weight
Grad req: transformer.h.7.input_layernorm.weight
Grad req: transformer.h.7.input_layernorm.bias
Grad req: transformer.h.8.self_attention.query_key_value.weight
Grad req: transformer.h.8.self_attention.dense.weight
Grad req: transformer.h.8.mlp.dense_h_to_4h.weight
Grad req: transformer.h.8.mlp.dense_4h_to_h.weight
Grad req: transformer.h.8.input_layernorm.weight
Grad req: transformer.h.8.input_layernorm.bias
Grad req: transformer.h.9.self_attention.query_key_value.weight
Grad req: transformer.h.9.self_attention.dense.weight
Grad req: transformer.h.9.mlp.dense_h_to_4h.weight
Grad req: transformer.h.9.mlp.dense_4h_to_h.weight
Grad req: transformer.h.9.input_layernorm.weight
Grad req: transformer.h.9.input_layernorm.bias
Grad req: transformer.h.10.self_attention.query_key_value.weight
Grad req: transformer.h.10.self_attention.dense.weight
Grad req: transformer.h.10.mlp.dense_h_to_4h.weight
Grad req: transformer.h.10.mlp.dense_4h_to_h.weight
Grad req: transformer.h.10.input_layernorm.weight
Grad req: transformer.h.10.input_layernorm.bias
Grad req: transformer.h.11.self_attention.query_key_value.weight
Grad req: transformer.h.11.self_attention.dense.weight
Grad req: transformer.h.11.mlp.dense_h_to_4h.weight
Grad req: transformer.h.11.mlp.dense_4h_to_h.weight
Grad req: transformer.h.11.input_layernorm.weight
Grad req: transformer.h.11.input_layernorm.bias
Grad req: transformer.h.12.self_attention.query_key_value.weight
Grad req: transformer.h.12.self_attention.dense.weight
Grad req: transformer.h.12.mlp.dense_h_to_4h.weight
Grad req: transformer.h.12.mlp.dense_4h_to_h.weight
Grad req: transformer.h.12.input_layernorm.weight
Grad req: transformer.h.12.input_layernorm.bias
Grad req: transformer.h.13.self_attention.query_key_value.weight
Grad req: transformer.h.13.self_attention.dense.weight
Grad req: transformer.h.13.mlp.dense_h_to_4h.weight
Grad req: transformer.h.13.mlp.dense_4h_to_h.weight
Grad req: transformer.h.13.input_layernorm.weight
Grad req: transformer.h.13.input_layernorm.bias
Grad req: transformer.h.14.self_attention.query_key_value.weight
Grad req: transformer.h.14.self_attention.dense.weight
Grad req: transformer.h.14.mlp.dense_h_to_4h.weight
Grad req: transformer.h.14.mlp.dense_4h_to_h.weight
Grad req: transformer.h.14.input_layernorm.weight
Grad req: transformer.h.14.input_layernorm.bias
Grad req: transformer.h.15.self_attention.query_key_value.weight
Grad req: transformer.h.15.self_attention.dense.weight
Grad req: transformer.h.15.mlp.dense_h_to_4h.weight
Grad req: transformer.h.15.mlp.dense_4h_to_h.weight
Grad req: transformer.h.15.input_layernorm.weight
Grad req: transformer.h.15.input_layernorm.bias
Grad req: transformer.h.16.self_attention.query_key_value.weight
Grad req: transformer.h.16.self_attention.dense.weight
Grad req: transformer.h.16.mlp.dense_h_to_4h.weight
Grad req: transformer.h.16.mlp.dense_4h_to_h.weight
Grad req: transformer.h.16.input_layernorm.weight
Grad req: transformer.h.16.input_layernorm.bias
Grad req: transformer.h.17.self_attention.query_key_value.weight
Grad req: transformer.h.17.self_attention.dense.weight
Grad req: transformer.h.17.mlp.dense_h_to_4h.weight
Grad req: transformer.h.17.mlp.dense_4h_to_h.weight
Grad req: transformer.h.17.input_layernorm.weight
Grad req: transformer.h.17.input_layernorm.bias
Grad req: transformer.h.18.self_attention.query_key_value.weight
Grad req: transformer.h.18.self_attention.dense.weight
Grad req: transformer.h.18.mlp.dense_h_to_4h.weight
Grad req: transformer.h.18.mlp.dense_4h_to_h.weight
Grad req: transformer.h.18.input_layernorm.weight
Grad req: transformer.h.18.input_layernorm.bias
Grad req: transformer.h.19.self_attention.query_key_value.weight
Grad req: transformer.h.19.self_attention.dense.weight
Grad req: transformer.h.19.mlp.dense_h_to_4h.weight
Grad req: transformer.h.19.mlp.dense_4h_to_h.weight
Grad req: transformer.h.19.input_layernorm.weight
Grad req: transformer.h.19.input_layernorm.bias
Grad req: transformer.h.20.self_attention.query_key_value.weight
Grad req: transformer.h.20.self_attention.dense.weight
Grad req: transformer.h.20.mlp.dense_h_to_4h.weight
Grad req: transformer.h.20.mlp.dense_4h_to_h.weight
Grad req: transformer.h.20.input_layernorm.weight
Grad req: transformer.h.20.input_layernorm.bias
Grad req: transformer.h.21.self_attention.query_key_value.weight
Grad req: transformer.h.21.self_attention.dense.weight
Grad req: transformer.h.21.mlp.dense_h_to_4h.weight
Grad req: transformer.h.21.mlp.dense_4h_to_h.weight
Grad req: transformer.h.21.input_layernorm.weight
Grad req: transformer.h.21.input_layernorm.bias
Grad req: transformer.h.22.self_attention.query_key_value.weight
Grad req: transformer.h.22.self_attention.dense.weight
Grad req: transformer.h.22.mlp.dense_h_to_4h.weight
Grad req: transformer.h.22.mlp.dense_4h_to_h.weight
Grad req: transformer.h.22.input_layernorm.weight
Grad req: transformer.h.22.input_layernorm.bias
Grad req: transformer.h.23.self_attention.query_key_value.weight
Grad req: transformer.h.23.self_attention.dense.weight
Grad req: transformer.h.23.mlp.dense_h_to_4h.weight
Grad req: transformer.h.23.mlp.dense_4h_to_h.weight
Grad req: transformer.h.23.input_layernorm.weight
Grad req: transformer.h.23.input_layernorm.bias
Grad req: transformer.h.24.self_attention.query_key_value.weight
Grad req: transformer.h.24.self_attention.dense.weight
Grad req: transformer.h.24.mlp.dense_h_to_4h.weight
Grad req: transformer.h.24.mlp.dense_4h_to_h.weight
Grad req: transformer.h.24.input_layernorm.weight
Grad req: transformer.h.24.input_layernorm.bias
Grad req: transformer.h.25.self_attention.query_key_value.weight
Grad req: transformer.h.25.self_attention.dense.weight
Grad req: transformer.h.25.mlp.dense_h_to_4h.weight
Grad req: transformer.h.25.mlp.dense_4h_to_h.weight
Grad req: transformer.h.25.input_layernorm.weight
Grad req: transformer.h.25.input_layernorm.bias
Grad req: transformer.h.26.self_attention.query_key_value.weight
Grad req: transformer.h.26.self_attention.dense.weight
Grad req: transformer.h.26.mlp.dense_h_to_4h.weight
Grad req: transformer.h.26.mlp.dense_4h_to_h.weight
Grad req: transformer.h.26.input_layernorm.weight
Grad req: transformer.h.26.input_layernorm.bias
Grad req: transformer.h.27.self_attention.query_key_value.weight
Grad req: transformer.h.27.self_attention.dense.weight
Grad req: transformer.h.27.mlp.dense_h_to_4h.weight
Grad req: transformer.h.27.mlp.dense_4h_to_h.weight
Grad req: transformer.h.27.input_layernorm.weight
Grad req: transformer.h.27.input_layernorm.bias
Grad req: transformer.h.28.self_attention.query_key_value.weight
Grad req: transformer.h.28.self_attention.dense.weight
Grad req: transformer.h.28.mlp.dense_h_to_4h.weight
Grad req: transformer.h.28.mlp.dense_4h_to_h.weight
Grad req: transformer.h.28.input_layernorm.weight
Grad req: transformer.h.28.input_layernorm.bias
Grad req: transformer.h.29.self_attention.query_key_value.weight
Grad req: transformer.h.29.self_attention.dense.weight
Grad req: transformer.h.29.mlp.dense_h_to_4h.weight
Grad req: transformer.h.29.mlp.dense_4h_to_h.weight
Grad req: transformer.h.29.input_layernorm.weight
Grad req: transformer.h.29.input_layernorm.bias
Grad req: transformer.h.30.self_attention.query_key_value.weight
Grad req: transformer.h.30.self_attention.dense.weight
Grad req: transformer.h.30.mlp.dense_h_to_4h.weight
Grad req: transformer.h.30.mlp.dense_4h_to_h.weight
Grad req: transformer.h.30.input_layernorm.weight
Grad req: transformer.h.30.input_layernorm.bias
Grad req: transformer.h.31.self_attention.query_key_value.weight
Grad req: transformer.h.31.self_attention.dense.weight
Grad req: transformer.h.31.mlp.dense_h_to_4h.weight
Grad req: transformer.h.31.mlp.dense_4h_to_h.weight
Grad req: transformer.h.31.input_layernorm.weight
Grad req: transformer.h.31.input_layernorm.bias
Grad req: transformer.ln_f.weight
Grad req: transformer.ln_f.bias
model.hf_device_map: {'transformer.word_embeddings': 0, 'lm_head': 0, 'transformer.h.0': 0, 'transformer.h.1': 0, 'transformer.h.2': 0, 'transformer.h.3': 0, 'transformer.h.4': 0, 'transformer.h.5': 0, 'transformer.h.6': 0, 'transformer.h.7': 1, 'transformer.h.8': 1, 'transformer.h.9': 1, 'transformer.h.10': 1, 'transformer.h.11': 1, 'transformer.h.12': 1, 'transformer.h.13': 1, 'transformer.h.14': 1, 'transformer.h.15': 1, 'transformer.h.16': 2, 'transformer.h.17': 2, 'transformer.h.18': 2, 'transformer.h.19': 2, 'transformer.h.20': 2, 'transformer.h.21': 2, 'transformer.h.22': 2, 'transformer.h.23': 2, 'transformer.h.24': 2, 'transformer.h.25': 3, 'transformer.h.26': 3, 'transformer.h.27': 3, 'transformer.h.28': 3, 'transformer.h.29': 3, 'transformer.h.30': 3, 'transformer.h.31': 3, 'transformer.ln_f': 3}
{'loss': 1.0229, 'learning_rate': 6.666666666666668e-08, 'epoch': 0.01}
{'loss': 1.0263, 'learning_rate': 1.3333333333333336e-07, 'epoch': 0.03}
{'loss': 1.015, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.04}
{'loss': 1.0257, 'learning_rate': 2.666666666666667e-07, 'epoch': 0.05}
{'loss': 1.0711, 'learning_rate': 3.3333333333333335e-07, 'epoch': 0.06}
{'loss': 0.9885, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.08}
{'loss': 1.018, 'learning_rate': 4.666666666666667e-07, 'epoch': 0.09}
{'loss': 1.0106, 'learning_rate': 5.333333333333335e-07, 'epoch': 0.1}
{'loss': 0.9972, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.12}
{'loss': 1.0331, 'learning_rate': 6.666666666666667e-07, 'epoch': 0.13}
{'loss': 0.9783, 'learning_rate': 7.333333333333334e-07, 'epoch': 0.14}
{'loss': 1.0103, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.15}
{'loss': 1.0087, 'learning_rate': 8.666666666666668e-07, 'epoch': 0.17}
{'loss': 1.0342, 'learning_rate': 9.333333333333334e-07, 'epoch': 0.18}
{'loss': 1.013, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.19}
{'loss': 1.0095, 'learning_rate': 1.066666666666667e-06, 'epoch': 0.2}
{'loss': 1.0319, 'learning_rate': 1.1333333333333334e-06, 'epoch': 0.22}
{'loss': 0.9989, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.23}
{'loss': 1.0544, 'learning_rate': 1.2666666666666669e-06, 'epoch': 0.24}
{'loss': 1.007, 'learning_rate': 1.3333333333333334e-06, 'epoch': 0.26}
{'loss': 1.0349, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.27}
{'loss': 0.9927, 'learning_rate': 1.4666666666666669e-06, 'epoch': 0.28}
{'loss': 1.0145, 'learning_rate': 1.5333333333333334e-06, 'epoch': 0.29}
{'loss': 1.0246, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.31}
{'loss': 0.9542, 'learning_rate': 1.6666666666666667e-06, 'epoch': 0.32}
{'loss': 1.0133, 'learning_rate': 1.7333333333333336e-06, 'epoch': 0.33}
{'loss': 0.9587, 'learning_rate': 1.8000000000000001e-06, 'epoch': 0.35}
{'loss': 0.9949, 'learning_rate': 1.8666666666666669e-06, 'epoch': 0.36}
{'loss': 1.0258, 'learning_rate': 1.9333333333333336e-06, 'epoch': 0.37}
{'loss': 0.9428, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.38}
{'loss': 0.9481, 'learning_rate': 2.0666666666666666e-06, 'epoch': 0.4}
{'loss': 0.9821, 'learning_rate': 2.133333333333334e-06, 'epoch': 0.41}
{'loss': 0.8906, 'learning_rate': 2.2e-06, 'epoch': 0.42}
{'loss': 0.9805, 'learning_rate': 2.266666666666667e-06, 'epoch': 0.44}
{'loss': 0.8835, 'learning_rate': 2.3333333333333336e-06, 'epoch': 0.45}
{'loss': 0.9028, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.46}
{'loss': 0.9742, 'learning_rate': 2.466666666666667e-06, 'epoch': 0.47}
{'loss': 0.894, 'learning_rate': 2.5333333333333338e-06, 'epoch': 0.49}
{'loss': 0.9368, 'learning_rate': 2.6e-06, 'epoch': 0.5}
{'loss': 0.8399, 'learning_rate': 2.666666666666667e-06, 'epoch': 0.51}
{'loss': 0.9116, 'learning_rate': 2.7333333333333336e-06, 'epoch': 0.52}
{'loss': 0.8851, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.54}
{'loss': 0.88, 'learning_rate': 2.866666666666667e-06, 'epoch': 0.55}
{'loss': 0.9015, 'learning_rate': 2.9333333333333338e-06, 'epoch': 0.56}
{'loss': 0.8262, 'learning_rate': 3e-06, 'epoch': 0.58}
{'loss': 0.9029, 'learning_rate': 3.066666666666667e-06, 'epoch': 0.59}
{'loss': 0.87, 'learning_rate': 3.133333333333334e-06, 'epoch': 0.6}
{'loss': 0.8467, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.61}
{'loss': 0.823, 'learning_rate': 3.266666666666667e-06, 'epoch': 0.63}
{'loss': 0.8489, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.64}
{'loss': 0.8136, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.65}
{'loss': 0.856, 'learning_rate': 3.4666666666666672e-06, 'epoch': 0.67}
{'loss': 0.7793, 'learning_rate': 3.5333333333333335e-06, 'epoch': 0.68}
{'loss': 0.8238, 'learning_rate': 3.6000000000000003e-06, 'epoch': 0.69}
{'loss': 0.8335, 'learning_rate': 3.6666666666666666e-06, 'epoch': 0.7}
{'loss': 0.8208, 'learning_rate': 3.7333333333333337e-06, 'epoch': 0.72}
{'loss': 0.8495, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.73}
{'loss': 0.7977, 'learning_rate': 3.866666666666667e-06, 'epoch': 0.74}
{'loss': 0.8116, 'learning_rate': 3.9333333333333335e-06, 'epoch': 0.76}
{'loss': 0.7911, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.77}
{'loss': 0.8131, 'learning_rate': 4.066666666666667e-06, 'epoch': 0.78}
{'loss': 0.8305, 'learning_rate': 4.133333333333333e-06, 'epoch': 0.79}
{'loss': 0.7748, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.81}
{'loss': 0.791, 'learning_rate': 4.266666666666668e-06, 'epoch': 0.82}
{'loss': 0.7618, 'learning_rate': 4.333333333333334e-06, 'epoch': 0.83}
{'loss': 0.7895, 'learning_rate': 4.4e-06, 'epoch': 0.84}
{'loss': 0.7986, 'learning_rate': 4.4666666666666665e-06, 'epoch': 0.86}
{'loss': 0.7991, 'learning_rate': 4.533333333333334e-06, 'epoch': 0.87}
{'loss': 0.7701, 'learning_rate': 4.600000000000001e-06, 'epoch': 0.88}
{'loss': 0.806, 'learning_rate': 4.666666666666667e-06, 'epoch': 0.9}
{'loss': 0.7705, 'learning_rate': 4.7333333333333335e-06, 'epoch': 0.91}
{'loss': 0.7919, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.92}
{'loss': 0.7426, 'learning_rate': 4.866666666666667e-06, 'epoch': 0.93}
{'loss': 0.7599, 'learning_rate': 4.933333333333334e-06, 'epoch': 0.95}
{'loss': 0.7961, 'learning_rate': 5e-06, 'epoch': 0.96}
{'loss': 0.7521, 'learning_rate': 5.0666666666666676e-06, 'epoch': 0.97}
{'loss': 0.7964, 'learning_rate': 5.133333333333334e-06, 'epoch': 0.99}
{'loss': 0.7881, 'learning_rate': 5.2e-06, 'epoch': 1.0}
{'loss': 0.7711, 'learning_rate': 5.2666666666666665e-06, 'epoch': 1.01}
{'loss': 0.7814, 'learning_rate': 5.333333333333334e-06, 'epoch': 1.02}
{'loss': 0.7589, 'learning_rate': 5.400000000000001e-06, 'epoch': 1.04}
{'loss': 0.7692, 'learning_rate': 5.466666666666667e-06, 'epoch': 1.05}
{'loss': 0.747, 'learning_rate': 5.533333333333334e-06, 'epoch': 1.06}
{'loss': 0.7634, 'learning_rate': 5.600000000000001e-06, 'epoch': 1.08}
{'loss': 0.7822, 'learning_rate': 5.666666666666667e-06, 'epoch': 1.09}
{'loss': 0.7422, 'learning_rate': 5.733333333333334e-06, 'epoch': 1.1}
{'loss': 0.7583, 'learning_rate': 5.8e-06, 'epoch': 1.11}
{'loss': 0.7849, 'learning_rate': 5.8666666666666675e-06, 'epoch': 1.13}
{'loss': 0.786, 'learning_rate': 5.933333333333335e-06, 'epoch': 1.14}
{'loss': 0.7576, 'learning_rate': 6e-06, 'epoch': 1.15}
{'loss': 0.7636, 'learning_rate': 6.066666666666667e-06, 'epoch': 1.16}
{'loss': 0.7366, 'learning_rate': 6.133333333333334e-06, 'epoch': 1.18}
{'loss': 0.7526, 'learning_rate': 6.200000000000001e-06, 'epoch': 1.19}
{'loss': 0.7375, 'learning_rate': 6.266666666666668e-06, 'epoch': 1.2}
{'loss': 0.7637, 'learning_rate': 6.333333333333333e-06, 'epoch': 1.22}
{'loss': 0.7624, 'learning_rate': 6.4000000000000006e-06, 'epoch': 1.23}
{'loss': 0.7853, 'learning_rate': 6.466666666666667e-06, 'epoch': 1.24}
{'loss': 0.768, 'learning_rate': 6.533333333333334e-06, 'epoch': 1.25}
{'loss': 0.7827, 'learning_rate': 6.600000000000001e-06, 'epoch': 1.27}
{'loss': 0.7513, 'learning_rate': 6.666666666666667e-06, 'epoch': 1.28}
